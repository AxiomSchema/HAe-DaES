{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rs4hxdC0J_n9"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hfU0uugcAST6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data.sampler import SequentialSampler\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from matplotlib.animation import FuncAnimation, PillowWriter\n",
        "from IPython.display import HTML\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktoBHCOxASKR",
        "outputId": "866f93e0-3ad3-4fc7-aa35-7f724ff86c05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda backend\n"
          ]
        }
      ],
      "source": [
        "# Training data, you can choose MNIST or Fashion MNIST\n",
        "DATASET_NAME = \"MNIST\" #@param [\"MNIST\", \"Fashion MNIST\"]\n",
        "# Generator architecture, you can choose cDCGAN or HAe/DaES\n",
        "ARCHITECHTURE = \"HAe/DaES\" #@param [\"VAE\", \"cDCGAN\", \"VAE and cDCGAN\", \"HAe/DaES\"]\n",
        "# batch size for training models\n",
        "BATCH_SIZE = 128 #@param {type:\"integer\"}\n",
        "# batch size for testing models\n",
        "TEST_BATCH_SIZE = 128 #@param {type:\"integer\"}\n",
        "# number of samples to be taken from VAE latent space\n",
        "NUM_SAMPLES = 10 #@param {type:\"integer\"}\n",
        "# scaling factor for sample standard deviation\n",
        "STD_SCALING = 7.5 #@param {type:\"number\"}\n",
        "# number of epochs over which to train the models\n",
        "NUM_EPOCHS = 200 #@param {type:\"integer\"}\n",
        "# number of epochs over which to train the models\n",
        "EPOCHS_BETWEEN_VAL = 100 #@param {type:\"integer\"}\n",
        "# Whether or not to shuffle the training and testing data\n",
        "# note: mutually exclusive with TRAIN_ON_SUBSET\n",
        "SHUFFLE = False #@param {type:\"boolean\"}\n",
        "#Whether or not to train on a subset of the MNIST training data\n",
        "# note: mutually exclusive with SHUFFLE\n",
        "TRAIN_ON_SUBSET = True #@param {type:\"boolean\"}\n",
        "#Size of subset on which to train\n",
        "SUBSET_SIZE = 600 #@param {type:\"integer\"}\n",
        "#Whether or not to pretrain the models before intangling them\n",
        "PRE_TRAIN = True #@param {type:\"boolean\"}\n",
        "# number of epochs over which to train the models\n",
        "NUM_PRETRAIN_EPOCHS = 200 #@param {type:\"integer\"}\n",
        "# Set the device\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using {DEVICE} backend\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7H5bcyk-ZAF"
      },
      "source": [
        "# Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tMST4DUo-4rs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5fdc24e-c2d3-4dfc-ce60-d9a96d430770"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./mnist/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 165054373.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./mnist/MNIST/raw/train-images-idx3-ubyte.gz to ./mnist/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./mnist/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 15576146.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./mnist/MNIST/raw/train-labels-idx1-ubyte.gz to ./mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./mnist/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 71770647.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./mnist/MNIST/raw/t10k-images-idx3-ubyte.gz to ./mnist/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 18074505.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./mnist/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./mnist/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor()\n",
        "                               ])\n",
        "\n",
        "if DATASET_NAME == \"MNIST\":\n",
        "    # download and load training set of MNISTtrain_data = datasets.MNIST(\"./mnist\", train=True, download=True, transform = transform)\n",
        "    train_data = datasets.MNIST(\"./mnist\", train=True, download=True, transform = transform)\n",
        "\n",
        "    # download and load test set of MNIST\n",
        "    test_data = datasets.MNIST(\"./mnist\", train=False, download=True, transform = transform)\n",
        "\n",
        "elif DATASET_NAME == \"Fashion MNIST\":\n",
        "    # download and load training set of Fashion MNIST\n",
        "    train_data = datasets.FashionMNIST(\"./fmnist\", train=True, download=True, transform = transform)\n",
        "\n",
        "    # download and load test set of Fashion MNIST\n",
        "    test_data = datasets.FashionMNIST(\"./fmnist\", train=False, download=True, transform = transform)\n",
        "else:\n",
        "  ValueError(f\"Please select valid dataset, {DATASET_NAME} is not supported\")\n",
        "\n",
        "if TRAIN_ON_SUBSET:\n",
        "  # Generate a list of random indices that covers the entire dataset\n",
        "  indices = torch.randperm(len(train_data))\n",
        "\n",
        "  # Select the first `SUBSET_SIZE` indices\n",
        "  selected_indices = indices[:SUBSET_SIZE]\n",
        "\n",
        "  # Define a sequential sampler using the first `SUBSET_SIZE` indices\n",
        "  sampler = SequentialSampler(selected_indices)\n",
        "else:\n",
        "  sampler = None\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=SHUFFLE, num_workers=2, drop_last=True, sampler=sampler)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(test_data, batch_size=TEST_BATCH_SIZE, shuffle=SHUFFLE, num_workers=2, drop_last=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "255UjzYF-Tra"
      },
      "source": [
        "# Variation Autoencoder Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tH5mMpCvAt8-"
      },
      "outputs": [],
      "source": [
        "ENCODED_DIM = 16 #@param {type:\"integer\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "E-UceQIFAwPn"
      },
      "outputs": [],
      "source": [
        "# https://medium.com/dataseries/convolutional-autoencoder-in-pytorch-on-mnist-dataset-d65145c132ac\n",
        "class Autoencoder(nn.Module):\n",
        "\n",
        "    def __init__(self, encoded_space_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        ### Encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, 3, stride=2, padding=1),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(8, 16, 3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(16, 32, 3, stride=2, padding=0),\n",
        "            nn.ReLU(True),\n",
        "            nn.Flatten(start_dim=1),\n",
        "            nn.Linear(3 * 3 * 32, 128),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(128, 256)\n",
        "        )\n",
        "        self.fc_mu = nn.Linear(256, encoded_space_dim)\n",
        "        self.fc_logvar = nn.Linear(256, encoded_space_dim)\n",
        "        ### Decoder\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(encoded_space_dim, 128),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(128, 3 * 3 * 32),\n",
        "            nn.ReLU(True),\n",
        "            nn.Unflatten(dim=1, unflattened_size=(32, 3, 3)),\n",
        "            nn.ConvTranspose2d(32, 16, 3, stride=2, output_padding=0),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(16, 8, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(8, 1, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        \"\"\"\n",
        "        :param mu: mean from the encoder's latent space\n",
        "        :param log_var: log variance from the encoder's latent space\n",
        "        \"\"\"\n",
        "        std = torch.exp(0.5*logvar)\n",
        "        z = torch.randn_like(std)\n",
        "        sample = mu + (z * std)\n",
        "        return sample\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded_x = self.encoder(x)\n",
        "        mu = self.fc_mu(encoded_x)\n",
        "        logvar = self.fc_logvar(encoded_x)\n",
        "        sample = self.reparameterize(mu, logvar)\n",
        "        reconstruction = self.decoder(sample)\n",
        "        return reconstruction, sample, mu, logvar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "d6ASA2Q4_Eba",
        "outputId": "f67ee773-b4ec-494f-f486-6dd949d2aea1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-98102dcc56eb>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mAE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mENCODED_DIM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mAE_total_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mAE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Total Trainable Parameters: {AE_total_params}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1156\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m   1157\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m-> 1158\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "AE = Autoencoder(ENCODED_DIM).to(DEVICE)\n",
        "print(AE)\n",
        "AE_total_params = sum(p.numel() for p in AE.parameters() if p.requires_grad)\n",
        "print(f\"Total Trainable Parameters: {AE_total_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_FkO0KyoKgY"
      },
      "source": [
        "# Discriminator Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QefVIquLoKOw"
      },
      "outputs": [],
      "source": [
        "# https://github.com/drc10723/GAN_design/blob/master/GAN_implementations/Conditional_DCGAN_MNIST.ipynb\n",
        "\n",
        "LABEL_SIZE = ENCODED_DIM\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "  \"\"\" D(x) \"\"\"\n",
        "  def __init__(self, label_size):\n",
        "    # initalize super module\n",
        "    super(Discriminator, self).__init__()\n",
        "\n",
        "    # creating layer for image input , input size : (batch_size, 1, 28, 28)\n",
        "    self.layer_x = nn.Sequential(nn.Conv2d(in_channels=1, out_channels=32,\n",
        "                                           kernel_size=4, stride=2, padding=1, bias=False),\n",
        "                                 # out size : (batch_size, 32, 14, 14)\n",
        "                                 nn.LeakyReLU(0.2, inplace=True),\n",
        "                                 # out size : (batch_size, 32, 14, 14)\n",
        "                                )\n",
        "\n",
        "    # creating layer for label input, input size : (batch_size, label_size, 28, 28)\n",
        "    self.layer_y = nn.Sequential(nn.Conv2d(in_channels=label_size, out_channels=32,\n",
        "                                           kernel_size=4, stride=2, padding=1, bias=False),\n",
        "                                 # out size : (batch_size, 32, 14, 14)\n",
        "                                 nn.LeakyReLU(0.2, inplace=True),\n",
        "                                 # out size : (batch_size, 32, 14, 14)\n",
        "                                 )\n",
        "\n",
        "    # layer for concat of image layer and label layer, input size : (batch_size, 64, 14, 14)\n",
        "    self.layer_xy = nn.Sequential(nn.Conv2d(in_channels=64, out_channels=128,\n",
        "                                            kernel_size=4, stride=2, padding=1, bias=False),\n",
        "                               # out size : (batch_size, 128, 7, 7)\n",
        "                               nn.BatchNorm2d(128),\n",
        "                               # out size : (batch_size, 128, 7, 7)\n",
        "                               nn.LeakyReLU(0.2, inplace=True),\n",
        "                               # out size : (batch_size, 128, 7, 7)\n",
        "                               nn.Conv2d(in_channels=128, out_channels=256,\n",
        "                                         kernel_size=3, stride=2, padding=0, bias=False),\n",
        "                               # out size : (batch_size, 256, 3, 3)\n",
        "                               nn.BatchNorm2d(256),\n",
        "                               # out size : (batch_size, 256, 3, 3)\n",
        "                               nn.LeakyReLU(0.2, inplace=True),\n",
        "                               # out size : (batch_size, 256, 3, 3)\n",
        "                               # Notice in below layer, we are using out channels as 1, we don't need to use Linear layer\n",
        "                               # Same is recommended in DCGAN paper also\n",
        "                               nn.Conv2d(in_channels=256, out_channels=1,\n",
        "                                         kernel_size=3, stride=1, padding=0, bias=False),\n",
        "                               # out size : (batch_size, 1, 1, 1)\n",
        "                               # sigmoid layer to convert in [0,1] range\n",
        "                               nn.Sigmoid()\n",
        "                               )\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    # size of x : (batch_size, 1, 28, 28)\n",
        "    x = self.layer_x(x)\n",
        "    # size of x : (batch_size, 32, 14, 14)\n",
        "\n",
        "    # size of y : (batch_size, LABEL_SIZE, 28, 28)\n",
        "    y = self.layer_y(y)\n",
        "    # size of y : (batch_size, 32, 14, 14)\n",
        "\n",
        "    # concat image layer and label layer output\n",
        "    xy = torch.cat([x,y], dim=1)\n",
        "    # size of xy : (batch_size, 64, 14, 14)\n",
        "    xy = self.layer_xy(xy)\n",
        "    # size of xy : (batch_size, 1, 1, 1)\n",
        "    xy = xy.view(xy.shape[0], -1)\n",
        "    # size of xy : (batch_size, 1)\n",
        "    return xy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wtp3XxqVou7x"
      },
      "outputs": [],
      "source": [
        "# Create the Discriminator\n",
        "D = Discriminator(LABEL_SIZE).to(DEVICE)\n",
        "print(D)\n",
        "D_total_params = sum(p.numel() for p in D.parameters() if p.requires_grad)\n",
        "print(f\"Total Trainable Parameters: {D_total_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6FpIur3oe5n"
      },
      "source": [
        "# Generator Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqlJgp4Ooq3W"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "  \"\"\" G(z) \"\"\"\n",
        "  def __init__(self, label_size, input_size=100):\n",
        "    # initalize super module\n",
        "    super(Generator, self).__init__()\n",
        "\n",
        "    # noise z input layer : (batch_size, 100, 1, 1)\n",
        "    self.layer_x = nn.Sequential(nn.ConvTranspose2d(in_channels=100, out_channels=128, kernel_size=3,\n",
        "                                                  stride=1, padding=0, bias=False),\n",
        "                                 # out size : (batch_size, 128, 3, 3)\n",
        "                                 nn.BatchNorm2d(128),\n",
        "                                 # out size : (batch_size, 128, 3, 3)\n",
        "                                 nn.ReLU(),\n",
        "                                 # out size : (batch_size, 128, 3, 3)\n",
        "                                )\n",
        "\n",
        "    # label input layer : (batch_size, label_size, 1, 1)\n",
        "    self.layer_y = nn.Sequential(nn.ConvTranspose2d(in_channels=label_size, out_channels=128, kernel_size=3,\n",
        "                                                  stride=1, padding=0, bias=False),\n",
        "                                 # out size : (batch_size, 128, 3, 3)\n",
        "                                 nn.BatchNorm2d(128),\n",
        "                                 # out size : (batch_size, 128, 3, 3)\n",
        "                                 nn.ReLU(),\n",
        "                                 # out size : (batch_size, 128, 3, 3)\n",
        "                                )\n",
        "\n",
        "    # noise z and label concat input layer : (batch_size, 256, 3, 3)\n",
        "    self.layer_xy = nn.Sequential(nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3,\n",
        "                                                  stride=2, padding=0, bias=False),\n",
        "                                  # out size : (batch_size, 128, 7, 7)\n",
        "                                  nn.BatchNorm2d(128),\n",
        "                                  # out size : (batch_size, 128, 7, 7)\n",
        "                                  nn.ReLU(),\n",
        "                                  # out size : (batch_size, 128, 7, 7)\n",
        "                                  nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=4,\n",
        "                                                      stride=2, padding=1, bias=False),\n",
        "                                  # out size : (batch_size, 64, 14, 14)\n",
        "                                  nn.BatchNorm2d(64),\n",
        "                                  # out size : (batch_size, 64, 14, 14)\n",
        "                                  nn.ReLU(),\n",
        "                                  # out size : (batch_size, 64, 14, 14)\n",
        "                                  nn.ConvTranspose2d(in_channels=64, out_channels=1, kernel_size=4,\n",
        "                                                      stride=2, padding=1, bias=False),\n",
        "                                  # out size : (batch_size, 1, 28, 28)\n",
        "                                  nn.Sigmoid()\n",
        "                                  # out size : (batch_size, 1, 28, 28)\n",
        "                                 )\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    # x size : (batch_size, 100)\n",
        "    x = x.view(x.shape[0], x.shape[1], 1, 1)\n",
        "    # x size : (batch_size, 100, 1, 1)\n",
        "    x = self.layer_x(x)\n",
        "    # x size : (batch_size, 128, 3, 3)\n",
        "\n",
        "    # y size : (batch_size, LABEL_SIZE)\n",
        "    y = y.view(y.shape[0], y.shape[1], 1, 1)\n",
        "    # y size : (batch_size, 100, 1, 1)\n",
        "    y = self.layer_y(y)\n",
        "    # y size : (batch_size, 128, 3, 3)\n",
        "\n",
        "    # concat x and y\n",
        "    xy = torch.cat([x,y], dim=1)\n",
        "    # xy size : (batch_size, 256, 3, 3)\n",
        "    xy = self.layer_xy(xy)\n",
        "    # xy size : (batch_size, 1, 28, 28)\n",
        "    return xy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhEliwSOox9n"
      },
      "outputs": [],
      "source": [
        "# Create the Generator\n",
        "G = Generator(LABEL_SIZE).to(DEVICE)\n",
        "print(G)\n",
        "G_total_params = sum(p.numel() for p in G.parameters() if p.requires_grad)\n",
        "print(f\"Total Trainable Parameters: {G_total_params}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75T1NtI4o6ha"
      },
      "source": [
        "# Weight Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rp4eELs4o9XM"
      },
      "outputs": [],
      "source": [
        "# custom weights initialization\n",
        "def weights_init(net):\n",
        "    classname = net.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(net.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(net.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(net.bias.data, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWBvFLuWo_Md"
      },
      "outputs": [],
      "source": [
        "# randomly initialize all weights to mean=0, stdev=0.2.\n",
        "D.apply(weights_init)\n",
        "G.apply(weights_init)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_fmu5lBUdPv"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBCjQePWDYY2"
      },
      "source": [
        "## Setup\n",
        "\n",
        "\n",
        "\n",
        "*   Value of beta1 hyperparameter in Adam optimizer has huge impact on stability of generator and DCGAN paper recommend 0.5 value.\n",
        "*   Recommended learning rate for Adam is 0.0002."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7ppD6ZOoQP-"
      },
      "outputs": [],
      "source": [
        "# size of latent vector z\n",
        "size_z = 100\n",
        "# number of discriminator steps for each generator step\n",
        "Ksteps = 1 #@param {type:\"integer\"}\n",
        "# number of discriminator steps for each Autoencoder step\n",
        "Jsteps = 1 #@param {type:\"integer\"}\n",
        "# learning rate of adam\n",
        "# DCGAN recommend 0.0002 lr\n",
        "Adam_lr = 0.0002 #@param {type:\"number\"}\n",
        "# DCGAN recommend 0.5\n",
        "Adam_beta1 = 0.5 #@param {type:\"number\"}\n",
        "# Scaling factor applied to Cross Entropy classification Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4I_fj3knUrW"
      },
      "outputs": [],
      "source": [
        "# We calculate Binary cross entropy loss\n",
        "discrimination_loss = nn.BCELoss()#reduction='sum')\n",
        "# Adam optimizer for generator\n",
        "D_optimizer = torch.optim.Adam(D.parameters(), lr=Adam_lr, betas=(Adam_beta1, 0.999))\n",
        "# Adam optimizer for discriminator\n",
        "G_optimizer = torch.optim.Adam(G.parameters(), lr=Adam_lr, betas=(Adam_beta1, 0.999))\n",
        "\n",
        "reconstruction_loss = nn.MSELoss()#reduction='sum')\n",
        "classifier_loss = nn.CrossEntropyLoss()#reduction='sum')\n",
        "AE_optimizer = torch.optim.Adam(AE.parameters(), lr=0.001)\n",
        "\n",
        "def kl_divergence(mu, logvar):\n",
        "  return -0.001 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyhZNliUrgCD"
      },
      "outputs": [],
      "source": [
        "# labels for training images x for Discriminator training\n",
        "labels_real = torch.ones((BATCH_SIZE, 1)).to(DEVICE)\n",
        "# labels for generated images G(z) for Discriminator training\n",
        "labels_fake = torch.zeros((BATCH_SIZE, 1)).to(DEVICE)\n",
        "# Fix noise for testing generator and visualization\n",
        "z_test = torch.randn(100, size_z).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXzZJazbbGMY"
      },
      "outputs": [],
      "source": [
        "img_size = 28\n",
        "\n",
        "# convert labels to onehot encoding\n",
        "onehot = torch.zeros(10, 10).scatter_(1, torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).view(10,1), 1).to(DEVICE)\n",
        "# reshape labels to image size, with number of labels as channel\n",
        "fill = torch.zeros([10, LABEL_SIZE, img_size, img_size]).to(DEVICE)\n",
        "#channel corresponding to label will be set one and all other zeros\n",
        "for i in range(10):\n",
        "  fill[i, i, :, :] = 1\n",
        "# create labels for testing generator\n",
        "test_y = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]*10).type(torch.LongTensor)\n",
        "# convert to one hot encoding\n",
        "test_Gy = onehot[test_y].to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8tkmt8iZxJH"
      },
      "source": [
        "## Save Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VW_zlw9HZxJH"
      },
      "outputs": [],
      "source": [
        "AE_path0 = \"./AE_checkpoint0\"\n",
        "torch.save({\n",
        "            'model_state_dict': AE.state_dict(),\n",
        "            'optimizer_state_dict': AE_optimizer.state_dict(),\n",
        "            }, AE_path0)\n",
        "\n",
        "G_path0 = \"./G_checkpoint0\"\n",
        "torch.save({\n",
        "            'model_state_dict': G.state_dict(),\n",
        "            'optimizer_state_dict': G_optimizer.state_dict(),\n",
        "            }, G_path0)\n",
        "\n",
        "D_path0 = \"./D_checkpoint0\"\n",
        "torch.save({\n",
        "            'model_state_dict': D.state_dict(),\n",
        "            'optimizer_state_dict': D_optimizer.state_dict(),\n",
        "            }, D_path0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOI9j0DHDTXR"
      },
      "source": [
        "## Train the VAE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UMa2DxLZo3Y"
      },
      "source": [
        "### Load checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiXQ_6FtZo3Z"
      },
      "outputs": [],
      "source": [
        "AE_checkpoint = torch.load(AE_path0)\n",
        "AE.load_state_dict(AE_checkpoint['model_state_dict'])\n",
        "AE_optimizer.load_state_dict(AE_checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkPVR0BTZo3Z"
      },
      "outputs": [],
      "source": [
        "G_checkpoint = torch.load(G_path0)\n",
        "G.load_state_dict(G_checkpoint['model_state_dict'])\n",
        "G_optimizer.load_state_dict(G_checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LubRA3YZo3Z"
      },
      "outputs": [],
      "source": [
        "D_checkpoint = torch.load(D_path0)\n",
        "D.load_state_dict(D_checkpoint['model_state_dict'])\n",
        "D_optimizer.load_state_dict(D_checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHnwdmXwZpY2"
      },
      "source": [
        "### Begin Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MXdbwWdDX98"
      },
      "outputs": [],
      "source": [
        "def get_class_examples(AE_dataloader):\n",
        "  # Initialize a tensor filled with zeros to store example images of each class\n",
        "  example_entries = torch.zeros(10, 1, 28, 28)\n",
        "  # Initialize a list to keep track of the number of seen classes\n",
        "  class_count = [0] * 10\n",
        "  # Iterate through each batch of images and classes in the dataloader\n",
        "  for images, classes in AE_dataloader:\n",
        "    # Iterate through each image and class in the batch\n",
        "    for i in range(images.shape[0]):\n",
        "      class_label = classes[i].item()\n",
        "      # If the class has not been seen yet, store the first image of that class\n",
        "      # in example_entries and increment the count for that class\n",
        "      if class_count[class_label] == 0:\n",
        "        example_entries[class_label] = images[i]\n",
        "        class_count[class_label] += 1\n",
        "        # If 10 classes have been counted, return example_entries\n",
        "        if sum(class_count) >= 10:\n",
        "          return example_entries\n",
        "  # Return example_entries even if there are less than 10 unique classes in the dataloader\n",
        "  return example_entries\n",
        "\n",
        "class_examples = get_class_examples(dataloader).to(DEVICE)\n",
        "val_class_examples = get_class_examples(testloader).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Skp2eNwcEocL"
      },
      "outputs": [],
      "source": [
        "def display_class_examples(tensors):\n",
        "  # Reshape the tensors to [28, 28]\n",
        "  images = [np.squeeze(tensor, axis=0).cpu() for tensor in tensors]\n",
        "\n",
        "  # Create a figure with a grid of subplots\n",
        "  fig, axes = plt.subplots(nrows=1, ncols=10, figsize=(20, 2.5))\n",
        "\n",
        "  # Flatten the axes array\n",
        "  axes = axes.flatten()\n",
        "\n",
        "  # Iterate over the images and add them to the subplots\n",
        "  for image, ax in zip(images, axes):\n",
        "    ax.imshow(image, cmap='gray')\n",
        "    ax.axis('off')\n",
        "\n",
        "  # Show the plot\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ut_jq8a6FmRp"
      },
      "outputs": [],
      "source": [
        "display_class_examples(class_examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHx-tqjCMRBL"
      },
      "outputs": [],
      "source": [
        "display_class_examples(val_class_examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfXsuMOg9ZYL"
      },
      "outputs": [],
      "source": [
        "def AE_epoch(dataloader, AE, AE_optimizer, reconstruction_loss, train=True):\n",
        "  epoch_AE_losses = []\n",
        "  epoch_AE_reconstruction_losses = []\n",
        "  for images, classes in dataloader:\n",
        "    images = Variable(images).to(DEVICE)\n",
        "    classes = Variable(classes).to(DEVICE)\n",
        "    ############################\n",
        "    # Forward Pass Through Autoencoder\n",
        "    ############################\n",
        "    reconstruction, _, mu, logvar = AE(images)\n",
        "    # Calculate the Mean Squared Error loss between the original and reconstructed image\n",
        "    AE_reconstruction_loss = reconstruction_loss(reconstruction, images)\n",
        "    AE_KLD = kl_divergence(mu, logvar)\n",
        "\n",
        "    ############################\n",
        "    # Update Autoencoder\n",
        "    ############################\n",
        "    AE_loss = AE_reconstruction_loss + AE_KLD\n",
        "\n",
        "    # save values for plots\n",
        "    epoch_AE_losses.append(AE_loss.item())\n",
        "    epoch_AE_reconstruction_losses.append(AE_reconstruction_loss.item())\n",
        "\n",
        "    if train:\n",
        "      # zero accumalted grads\n",
        "      AE_optimizer.zero_grad()\n",
        "      # do backward pass\n",
        "      AE_loss.backward()\n",
        "      # update autoencoder model\n",
        "      AE_optimizer.step()\n",
        "\n",
        "  ############################\n",
        "  # Log\n",
        "  ############################\n",
        "  epoch_AE_loss = sum(epoch_AE_losses)/ len(epoch_AE_losses)\n",
        "  epoch_AE_reconstruction_loss = sum(epoch_AE_reconstruction_losses)/ len(epoch_AE_reconstruction_losses)\n",
        "  return epoch_AE_loss, epoch_AE_reconstruction_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dh0T5zUlADiV"
      },
      "outputs": [],
      "source": [
        "if ARCHITECHTURE == 'VAE' or ARCHITECHTURE == 'cDCGAN' or \\\n",
        "   ARCHITECHTURE == 'VAE and cDCGAN':\n",
        "  EPOCHS = NUM_EPOCHS\n",
        "else:\n",
        "  EPOCHS = NUM_PRETRAIN_EPOCHS\n",
        "\n",
        "if PRE_TRAIN or ARCHITECHTURE == 'VAE' or ARCHITECHTURE == 'VAE and cDCGAN':\n",
        "  AE_losses = []\n",
        "  reconstruction_losses = []\n",
        "\n",
        "  val_AE_losses = []\n",
        "  val_reconstruction_losses = []\n",
        "\n",
        "  counter = 0\n",
        "  for epoch in range(EPOCHS):\n",
        "    ############################\n",
        "    # Train\n",
        "    ############################\n",
        "    losses = AE_epoch(dataloader, AE, AE_optimizer,\n",
        "                      reconstruction_loss, train=True)\n",
        "\n",
        "    AE_losses.append(losses[0])\n",
        "    reconstruction_losses.append(losses[1])\n",
        "\n",
        "    ############################\n",
        "    # Display\n",
        "    ############################\n",
        "    AE.eval()\n",
        "    print('epoch [{}/{}], loss:{:.3f}, reconstruction:{:.3f}'.format(epoch+1, EPOCHS,\n",
        "                                                          AE_losses[-1],\n",
        "                                                          reconstruction_losses[-1]))\n",
        "    with torch.no_grad():\n",
        "      reconstructed_class_examples, _, _, _ = AE(class_examples)\n",
        "      display_class_examples(class_examples)\n",
        "      display_class_examples(reconstructed_class_examples)\n",
        "\n",
        "      if counter % EPOCHS_BETWEEN_VAL == 0:\n",
        "        ############################\n",
        "        # Validate\n",
        "        ############################\n",
        "        val_losses = AE_epoch(testloader, AE, AE_optimizer,\n",
        "                              reconstruction_loss, train=False)\n",
        "\n",
        "        val_AE_losses.append(val_losses[0])\n",
        "        val_reconstruction_losses.append(val_losses[1])\n",
        "\n",
        "        ############################\n",
        "        # Display\n",
        "        ############################\n",
        "        print('Validation loss:{:.3f}, reconstruction:{:.3f}'.format(val_AE_losses[-1],\n",
        "                                                          val_reconstruction_losses[-1]))\n",
        "        reconstructed_val_class_examples, _, _, _ = AE(val_class_examples)\n",
        "        display_class_examples(val_class_examples)\n",
        "        display_class_examples(reconstructed_val_class_examples)\n",
        "\n",
        "    AE.train()\n",
        "    counter += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3u9vyIYsv_At"
      },
      "source": [
        "### Visualizing VAE Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XXW6PCcPiyt"
      },
      "outputs": [],
      "source": [
        "if PRE_TRAIN or ARCHITECHTURE == 'VAE' or ARCHITECHTURE == 'VAE and cDCGAN':\n",
        "  def plot_losses(training_losses, validation_losses, offset_factor):\n",
        "      x_vals = [i * offset_factor for i in range(len(validation_losses))]\n",
        "      plt.plot(x_vals, validation_losses, label='Validation Loss')\n",
        "      plt.plot(range(len(training_losses)), training_losses,\n",
        "              label='Training Loss')\n",
        "      plt.xlabel('Iteration')\n",
        "      plt.ylabel('Loss')\n",
        "      plt.legend()\n",
        "      plt.show()\n",
        "  plot_losses(AE_losses, val_AE_losses, EPOCHS_BETWEEN_VAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCcb3Tok68w9"
      },
      "outputs": [],
      "source": [
        "def sort_data(data, labels):\n",
        "  \"\"\"\n",
        "  Sorts the data and labels tensors by label.\n",
        "  Args:\n",
        "    data: a tensor of data samples.\n",
        "    labels: a tensor of corresponding labels.\n",
        "  Returns:\n",
        "    A list of sorted data tensors, where the i-th tensor in the list contains all data samples with label i.\n",
        "  \"\"\"\n",
        "  # Create a list of 10 empty tensors to store the sorted data\n",
        "  sorted_data = [torch.empty((0, data.shape[1])).cuda() for _ in range(10)]\n",
        "\n",
        "  # Iterate through the data and labels tensors\n",
        "  for d, l in zip(data, labels):\n",
        "    # Append the data entry to the appropriate tensor in the sorted_data list\n",
        "    sorted_data[l] = torch.cat((sorted_data[l], d.unsqueeze(0)))\n",
        "\n",
        "  return sorted_data\n",
        "\n",
        "if PRE_TRAIN or ARCHITECHTURE == 'VAE' or ARCHITECHTURE == 'VAE and cDCGAN':\n",
        "  encoded_class_sums = [torch.zeros(ENCODED_DIM).cuda() for _ in range(10)]\n",
        "  class_totals = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "  for img, classes in dataloader:\n",
        "    images = Variable(img).cuda()\n",
        "    labels = Variable(classes).cuda()\n",
        "    with torch.no_grad():\n",
        "      _, encoded_images, _, _ = AE(images)\n",
        "    # Sort the encoded images by label\n",
        "    sorted_encoded_images = sort_data(encoded_images, labels)\n",
        "    # Iterate through the sorted encoded image tensors\n",
        "    for i in range(len(sorted_encoded_images)):\n",
        "      # Add the sum of the encoded images in the current tensor to the encoded class sum for this label\n",
        "      encoded_class_sums[i] = torch.add(torch.sum(sorted_encoded_images[i], 0),\n",
        "                                        encoded_class_sums[i])\n",
        "\n",
        "      # Increment the class total for this label by the number of encoded images in the current tensor\n",
        "      class_totals[i] += len(sorted_encoded_images[i])\n",
        "\n",
        "  # Initialize a list to store the class encodings\n",
        "  class_encodings = [torch.zeros(ENCODED_DIM) for _ in range(10)]\n",
        "\n",
        "  # Iterate through the encoded class sums\n",
        "  for l in range(len(encoded_class_sums)):\n",
        "    # Calculate the class encoding as the mean of the encoded images for each class\n",
        "    class_encodings[l] = torch.div(encoded_class_sums[l], class_totals[l]).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YJmbcnh7C_i"
      },
      "outputs": [],
      "source": [
        "if PRE_TRAIN or ARCHITECHTURE == 'VAE' or ARCHITECHTURE == 'VAE and cDCGAN':\n",
        "  tensors = []\n",
        "  for i in range(len(class_encodings)):\n",
        "    with torch.no_grad():\n",
        "      tensors.append(AE.decoder(class_encodings[i]).detach())\n",
        "  # Reshape the tensors to [28, 28]\n",
        "  images = [np.squeeze(tensor).cpu() for tensor in tensors]\n",
        "\n",
        "  # Create a figure with a grid of subplots\n",
        "  fig, axes = plt.subplots(nrows=1, ncols=10, figsize=(20, 2.5))\n",
        "\n",
        "  # Flatten the axes array\n",
        "  axes = axes.flatten()\n",
        "\n",
        "  # Iterate over the images and add them to the subplots\n",
        "  for image, ax in zip(images, axes):\n",
        "      ax.imshow(image, cmap='gray')\n",
        "      ax.axis('off')\n",
        "\n",
        "  # Show the plot\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LQKvyUu7Leb"
      },
      "outputs": [],
      "source": [
        "if PRE_TRAIN or ARCHITECHTURE == 'VAE' or ARCHITECHTURE == 'VAE and cDCGAN':\n",
        "  image_features = torch.stack(class_encodings, dim=0).squeeze()\n",
        "  image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "  similarity = image_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
        "  plt.figure(figsize=(20, 14))\n",
        "  plt.imshow(similarity)\n",
        "  plt.yticks(range(10), fontsize=18)\n",
        "  plt.xticks(range(10), fontsize=18)\n",
        "  for x in range(similarity.shape[1]):\n",
        "      for y in range(similarity.shape[0]):\n",
        "          plt.text(x, y, f\"{similarity[y, x]:.2f}\", ha=\"center\", va=\"center\",\n",
        "                   size=12)\n",
        "\n",
        "  for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
        "    plt.gca().spines[side].set_visible(False)\n",
        "\n",
        "  plt.xlim([-0.5, 10 - 0.5])\n",
        "  plt.ylim([9 + 0.5, -2])\n",
        "\n",
        "  plt.title(\"Cosine similarity between averaged image features\", size=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bb51kk8viIkW"
      },
      "outputs": [],
      "source": [
        "def show_image(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "AE.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    # calculate mean and std of latent code, generated takining in test images as inputs\n",
        "    images, labels = next(iter(dataloader))\n",
        "    images = images.to(DEVICE)\n",
        "    _, latent, _, _ = AE(images)\n",
        "    latent = latent.cpu()\n",
        "\n",
        "    mean = latent.mean(dim=0)\n",
        "    std = (latent - mean).pow(2).mean(dim=0).sqrt()\n",
        "\n",
        "    # sample latent vectors from the normal distribution\n",
        "    latent = torch.randn(128, ENCODED_DIM)*std + mean\n",
        "\n",
        "    # reconstruct images from the random latent vectors\n",
        "    latent = latent.to(DEVICE)\n",
        "    img_recon = AE.decoder(latent)\n",
        "    img_recon = img_recon.cpu()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(20, 8.5))\n",
        "    show_image(torchvision.utils.make_grid(img_recon[:100],10,5))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hf4NcujXRoCR"
      },
      "outputs": [],
      "source": [
        "AE.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    # calculate mean and std of latent code, generated takining in test images as inputs\n",
        "    images, labels = next(iter(dataloader))\n",
        "    images = images.to(DEVICE)\n",
        "    _, latent, mu, logvar = AE(images)\n",
        "    latent = latent.cpu()\n",
        "\n",
        "    std = 10 * torch.exp(0.5*logvar)\n",
        "\n",
        "    z = torch.randn_like(std)\n",
        "\n",
        "    # sample latent vectors from the normal distribution\n",
        "    latent = mu + (z * std)\n",
        "\n",
        "    # reconstruct images from the random latent vectors\n",
        "    latent = latent.to(DEVICE)\n",
        "    img_recon = AE.decoder(latent)\n",
        "    img_recon = img_recon.cpu()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(20, 8.5))\n",
        "    show_image(torchvision.utils.make_grid(img_recon[:100],10,5))\n",
        "    plt.show()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(20, 8.5))\n",
        "    show_image(torchvision.utils.make_grid(images[:100].cpu(),10,5))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxvBuh5whuVz"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    # calculate mean and std of latent code, generated takining in test images as inputs\n",
        "    images, labels = next(iter(dataloader))\n",
        "    images = images.to(DEVICE)\n",
        "    _, latent, mu, logvar = AE(images)\n",
        "    latent = latent.cpu()\n",
        "\n",
        "    std = 10 * torch.exp(0.5*logvar)\n",
        "\n",
        "    z = torch.randn_like(std)\n",
        "\n",
        "    # sample latent vectors from the normal distribution\n",
        "    latent = mu[0] + (z * std[0])\n",
        "\n",
        "    # reconstruct images from the random latent vectors\n",
        "    latent = latent.to(DEVICE)\n",
        "    img_recon = AE.decoder(latent)\n",
        "    img_recon = img_recon.cpu()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(20, 8.5))\n",
        "    show_image(torchvision.utils.make_grid(img_recon[:100],10,5))\n",
        "    plt.show()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(20, 8.5))\n",
        "    show_image(torchvision.utils.make_grid(images[0].cpu(),10,5))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbk6XBtY9bYk"
      },
      "outputs": [],
      "source": [
        "AE.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    # calculate mean and std of latent code, generated takining in test images as inputs\n",
        "    images, labels = next(iter(testloader))\n",
        "    images = images.to(DEVICE)\n",
        "    _, latent, mu, log_var = AE(images)\n",
        "    latent = latent.cpu()\n",
        "\n",
        "    mean = mu.mean(dim=0).cpu()\n",
        "    std = torch.exp(0.5*log_var).mean(dim=0).cpu()\n",
        "\n",
        "    # sample latent vectors from the normal distribution\n",
        "    latent = torch.randn(128, ENCODED_DIM)*std + mean\n",
        "\n",
        "    # reconstruct images from the random latent vectors\n",
        "    latent = latent.to(DEVICE)\n",
        "    img_recon = AE.decoder(latent)\n",
        "    img_recon = img_recon.cpu()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(20, 8.5))\n",
        "    show_image(torchvision.utils.make_grid(img_recon[:100],10,5))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1np49EeOeoz"
      },
      "outputs": [],
      "source": [
        "if PRE_TRAIN or ARCHITECHTURE == 'VAE' or ARCHITECHTURE == 'VAE and cDCGAN':\n",
        "  with torch.no_grad():\n",
        "    for i in range(10):\n",
        "      for j in range(10):\n",
        "        tensor1 = class_examples[i]\n",
        "        tensor2 = class_examples[j]\n",
        "        n=20\n",
        "        interpolated_tensors = []\n",
        "        for k in range(n):\n",
        "            alpha = k / (n - 1)\n",
        "            interpolated_tensor = (1 - alpha) * tensor1 + alpha * tensor2\n",
        "            interpolated_tensors.append(interpolated_tensor)\n",
        "\n",
        "        # Convert the list of tensors to a single tensor\n",
        "        interpolated_tensor = torch.stack(interpolated_tensors)\n",
        "        reconstructed, encoded, _, _ = AE(interpolated_tensor)\n",
        "        # Reshape the tensors to [28, 28]\n",
        "        images = [np.squeeze(tensor, axis=0).cpu() for tensor in reconstructed]\n",
        "\n",
        "        # Create a figure with a grid of subplots\n",
        "        fig, axes = plt.subplots(nrows=1, ncols=20, figsize=(20, 2.5))\n",
        "\n",
        "        # Flatten the axes array\n",
        "        axes = axes.flatten()\n",
        "\n",
        "        # Iterate over the images and add them to the subplots\n",
        "        for image, ax in zip(images, axes):\n",
        "          ax.imshow(image, cmap='gray')\n",
        "          ax.axis('off')\n",
        "        # Show the plot\n",
        "        plt.show()\n",
        "        print(\"------------------------------------------------------------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aMLcQbIZ_dg"
      },
      "source": [
        "### Save Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdiR9MTYYIB_"
      },
      "outputs": [],
      "source": [
        "AE_path1 = \"./AE_checkpoint1\"\n",
        "torch.save({\n",
        "            'model_state_dict': AE.state_dict(),\n",
        "            'optimizer_state_dict': AE_optimizer.state_dict(),\n",
        "            }, AE_path1)\n",
        "\n",
        "G_path1 = \"./G_checkpoint1\"\n",
        "torch.save({\n",
        "            'model_state_dict': G.state_dict(),\n",
        "            'optimizer_state_dict': G_optimizer.state_dict(),\n",
        "            }, G_path1)\n",
        "\n",
        "D_path1 = \"./D_checkpoint1\"\n",
        "torch.save({\n",
        "            'model_state_dict': D.state_dict(),\n",
        "            'optimizer_state_dict': D_optimizer.state_dict(),\n",
        "            }, D_path1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcA6UALJDOoJ"
      },
      "source": [
        "## Train the cDCGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxEbStFHZIGl"
      },
      "source": [
        "### Load checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPO-ivEzZIGr"
      },
      "outputs": [],
      "source": [
        "AE_checkpoint = torch.load(AE_path1)\n",
        "AE.load_state_dict(AE_checkpoint['model_state_dict'])\n",
        "AE_optimizer.load_state_dict(AE_checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sw8WcQqEZIGr"
      },
      "outputs": [],
      "source": [
        "G_checkpoint = torch.load(G_path1)\n",
        "G.load_state_dict(G_checkpoint['model_state_dict'])\n",
        "G_optimizer.load_state_dict(G_checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUypYa0eZIGr"
      },
      "outputs": [],
      "source": [
        "D_checkpoint = torch.load(D_path1)\n",
        "D.load_state_dict(D_checkpoint['model_state_dict'])\n",
        "D_optimizer.load_state_dict(D_checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-J4RPdL_ZJHB"
      },
      "source": [
        "### Begin Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjfQV4-sUM55"
      },
      "outputs": [],
      "source": [
        "def GAN_epoch(dataloader, AE, D, D_optimizer, G, G_optimizer,\n",
        "              discrimination_loss, reconstruction_loss, train=True):\n",
        "  torch.autograd.set_detect_anomaly(True)\n",
        "  epoch_D_losses = []\n",
        "  epoch_D_x_losses = []\n",
        "  epoch_D_z_losses = []\n",
        "  epoch_D_y_losses = []\n",
        "  epoch_Dx = []\n",
        "\n",
        "  epoch_G_losses = []\n",
        "  epoch_G_z_losses = []\n",
        "  epoch_G_z_reconstruction_losses = []\n",
        "  epoch_G_y_losses = []\n",
        "  epoch_DGz = []\n",
        "  epoch_DGy = []\n",
        "\n",
        "  step = 0\n",
        "  # iterate through data loader generator object\n",
        "  for images, classes in dataloader:\n",
        "    images = Variable(images).to(DEVICE)\n",
        "    ############################\n",
        "    # Forward Pass Through Autoencoder\n",
        "    ############################\n",
        "    _, encoded_images, mu, logvar = AE(images)\n",
        "\n",
        "    ############################\n",
        "    # Forward Pass Through Generator\n",
        "    ############################\n",
        "    # create latent vector z from normal distribution\n",
        "    z = torch.randn(BATCH_SIZE, size_z).to(DEVICE)\n",
        "    # generate image\n",
        "    fake_images = G(z, encoded_images.detach())\n",
        "\n",
        "    ############################\n",
        "    # Sample From AE's Latent Space\n",
        "    ###########################\n",
        "    mean = mu.repeat(NUM_SAMPLES, 1)\n",
        "    std = STD_SCALING * torch.exp(0.5*logvar).repeat(NUM_SAMPLES, 1)\n",
        "    y = torch.randn_like(std)\n",
        "\n",
        "    # sample latent vectors from the normal distribution\n",
        "    samples = mean + (y * std)\n",
        "\n",
        "    # reconstruct images from the random latent vectors\n",
        "    samples = samples.to(DEVICE)\n",
        "    reconstructed_samples = AE.decoder(samples.detach())\n",
        "\n",
        "    ############################\n",
        "    # Forward Pass Latent Samples Through Generator\n",
        "    ############################\n",
        "    # generate image\n",
        "    fake_samples = G(z.repeat(NUM_SAMPLES, 1), samples.detach())\n",
        "\n",
        "    ############################\n",
        "    # Calculate Discriminator loss on real images\n",
        "    ############################\n",
        "    # D_x shape will be (batch_size, ENCODED_DIM, 28, 28)\n",
        "    D_x = encoded_images.unsqueeze(2).unsqueeze(3).detach().repeat(1, 1, img_size, img_size)\n",
        "    # forward pass D(x)\n",
        "    x_preds = D(images, D_x)\n",
        "    # calculate loss log(D(x))\n",
        "    D_x_loss = discrimination_loss(x_preds, labels_real)\n",
        "\n",
        "    ############################\n",
        "    # Calculate Discriminator loss on fake images\n",
        "    ############################\n",
        "    # forward pass D(G(z))\n",
        "    z_preds = D(fake_images.detach(), D_x)\n",
        "    # calculate loss log(1 - D(G(z)))\n",
        "    D_z_loss = discrimination_loss(z_preds, labels_fake)\n",
        "\n",
        "    ############################\n",
        "    # Calculate Discriminator loss on fake latent samples\n",
        "    ############################\n",
        "    # D_y shape will be (batch_size, ENCODED_DIM, 28, 28)\n",
        "    D_y = D_x.repeat(NUM_SAMPLES, 1, 1, 1)\n",
        "    # forward pass D(G(z,y))\n",
        "    y_preds = D(fake_samples.detach(), D_y)\n",
        "    # calculate loss log(1 - D(G(z,y)))\n",
        "    D_y_loss = discrimination_loss(y_preds, labels_fake.repeat(NUM_SAMPLES, 1))\n",
        "\n",
        "    ############################\n",
        "    # Update D network\n",
        "    ############################\n",
        "    D_loss = D_x_loss + (D_z_loss + D_y_loss)/2\n",
        "\n",
        "    # save values for plots\n",
        "    epoch_D_losses.append(D_loss.item())\n",
        "    epoch_D_x_losses.append(D_x_loss.item())\n",
        "    epoch_D_z_losses.append(D_z_loss.item()/2)\n",
        "    epoch_D_y_losses.append(D_y_loss.item()/2)\n",
        "    epoch_Dx.append(x_preds.mean().item())\n",
        "\n",
        "    if train:\n",
        "      # zero accumalted grads\n",
        "      D.zero_grad()\n",
        "      # do backward pass\n",
        "      D_loss.backward()\n",
        "      # update discriminator model\n",
        "      D_optimizer.step()\n",
        "\n",
        "    ############################\n",
        "    # Update G network\n",
        "    ############################\n",
        "    # if Ksteps of Discriminator training are done, update generator\n",
        "    if step % Ksteps == 0:\n",
        "      # As we done one step of discriminator, again calculate D(G(z))\n",
        "      # forward pass D(G(z))\n",
        "      z_out = D(fake_images, D_x)\n",
        "      # calculate loss log(D(G(z)))\n",
        "      G_z_loss = discrimination_loss(z_out, labels_real)\n",
        "      # Calculate the Mean Squared Error loss between the original and generated image\n",
        "      G_z_reconstruction_loss = reconstruction_loss(fake_images, images)\n",
        "\n",
        "      # forward pass D(G(z))\n",
        "      y_out = D(fake_samples, D_y)\n",
        "      # calculate loss log(D(G(z)))\n",
        "      G_y_loss = discrimination_loss(y_out, labels_real.repeat(NUM_SAMPLES, 1))\n",
        "      G_y_reconstruction_loss = reconstruction_loss(fake_samples, reconstructed_samples)\n",
        "\n",
        "      G_loss = G_z_loss + G_y_loss# + G_y_reconstruction_loss# + G_z_reconstruction_loss\n",
        "\n",
        "      # save values for plots\n",
        "      epoch_G_losses.append(G_loss.item())\n",
        "      epoch_G_z_losses.append(G_z_loss.item())\n",
        "      epoch_G_z_reconstruction_losses.append(G_z_reconstruction_loss.item())\n",
        "      epoch_G_y_losses.append(G_y_loss.item())\n",
        "      epoch_DGz.append(z_out.mean().item())\n",
        "      epoch_DGy.append(y_out.mean().item())\n",
        "\n",
        "      if train:\n",
        "        # zero accumalted grads\n",
        "        G.zero_grad()\n",
        "        # do backward pass\n",
        "        G_loss.backward()\n",
        "        # update generator model\n",
        "        G_optimizer.step()\n",
        "    step += 1\n",
        "\n",
        "  ############################\n",
        "  # Log\n",
        "  ############################\n",
        "  epoch_D_loss = sum(epoch_D_losses)/ len(epoch_D_losses)\n",
        "  epoch_D_x_loss = sum(epoch_D_x_losses)/ len(epoch_D_x_losses)\n",
        "  epoch_D_z_loss = sum(epoch_D_z_losses)/ len(epoch_D_z_losses)\n",
        "  epoch_D_y_loss = sum(epoch_D_y_losses)/ len(epoch_D_y_losses)\n",
        "  epoch_Dx = sum(epoch_Dx)/ len(epoch_Dx)\n",
        "\n",
        "  epoch_G_loss = sum(epoch_G_losses)/ len(epoch_G_losses)\n",
        "  epoch_G_z_loss = sum(epoch_G_z_losses)/ len(epoch_G_z_losses)\n",
        "  epoch_G_z_reconstruction_loss = sum(epoch_G_z_reconstruction_losses)/ len(epoch_G_z_reconstruction_losses)\n",
        "  epoch_G_y_loss = sum(epoch_G_y_losses)/ len(epoch_G_y_losses)\n",
        "  epoch_DGz = sum(epoch_DGz)/ len(epoch_DGz)\n",
        "  epoch_DGy = sum(epoch_DGy)/ len(epoch_DGy)\n",
        "\n",
        "  return epoch_D_loss, epoch_D_x_loss, epoch_D_z_loss, epoch_D_y_loss, epoch_Dx, \\\n",
        "         epoch_G_loss, epoch_G_z_loss, epoch_G_z_reconstruction_loss, epoch_G_y_loss, \\\n",
        "         epoch_DGz, epoch_DGy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F51ZTy7ukQni"
      },
      "outputs": [],
      "source": [
        "_, encoded_class_examples, _, _ = AE(class_examples)\n",
        "class_examples_z = torch.randn(10, size_z).to(DEVICE)\n",
        "\n",
        "_, encoded_val_class_examples, _, _ = AE(val_class_examples)\n",
        "val_class_examples_z = torch.randn(10, size_z).to(DEVICE)\n",
        "\n",
        "if PRE_TRAIN or ARCHITECHTURE == 'cDCGAN' or ARCHITECHTURE == 'VAE and cDCGAN':\n",
        "  # List of values, which will be used for plotting purpose\n",
        "  D_losses = []\n",
        "  D_x_losses = []\n",
        "  D_z_losses = []\n",
        "  D_y_losses = []\n",
        "  Dx_values = []\n",
        "\n",
        "  G_losses = []\n",
        "  G_z_losses = []\n",
        "  G_z_reconstruction_losses = []\n",
        "  G_y_losses = []\n",
        "  DGz_values = []\n",
        "  DGy_values = []\n",
        "\n",
        "  val_D_losses = []\n",
        "  val_D_x_losses = []\n",
        "  val_D_z_losses = []\n",
        "  val_D_y_losses = []\n",
        "  val_Dx_values = []\n",
        "\n",
        "  val_G_losses = []\n",
        "  val_G_z_losses = []\n",
        "  val_G_z_reconstruction_losses = []\n",
        "  val_G_y_losses = []\n",
        "  val_DGz_values = []\n",
        "  val_DGy_values = []\n",
        "\n",
        "  counter = 0\n",
        "  AE.eval()\n",
        "  for epoch in range(EPOCHS):\n",
        "    ############################\n",
        "    # Train\n",
        "    ############################\n",
        "    losses = GAN_epoch(dataloader, AE, D, D_optimizer, G, G_optimizer,\n",
        "                                  discrimination_loss, reconstruction_loss,\n",
        "                                  train=True)\n",
        "\n",
        "    D_losses.append(losses[0])\n",
        "    D_x_losses.append(losses[1])\n",
        "    D_z_losses.append(losses[2])\n",
        "    D_y_losses.append(losses[3])\n",
        "    Dx_values.append(losses[4])\n",
        "\n",
        "    G_losses.append(losses[5])\n",
        "    G_z_losses.append(losses[6])\n",
        "    G_z_reconstruction_losses.append(losses[7])\n",
        "    G_y_losses.append(losses[8])\n",
        "    DGz_values.append(losses[9])\n",
        "    DGy_values.append(losses[10])\n",
        "\n",
        "    ############################\n",
        "    # Display\n",
        "    ############################\n",
        "    G.eval()\n",
        "    D.eval()\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS} Discriminator Loss {D_losses[-1]:.3f} \"\n",
        "        + f\"Generator Loss {G_losses[-1]:.3f} \"\n",
        "        + f\"D(x) {Dx_values[-1]:.3f} D(G(z)) {DGz_values[-1]:.3f} D(G(y)) {DGy_values[-1]:.3f}\")\n",
        "    with torch.no_grad():\n",
        "      fake_class_examples = G(class_examples_z, encoded_class_examples)\n",
        "      display_class_examples(class_examples)\n",
        "      display_class_examples(fake_class_examples)\n",
        "      _, latent, mu, logvar = AE(class_examples)\n",
        "      std = STD_SCALING * torch.exp(0.5*logvar)\n",
        "      z = torch.randn_like(std)\n",
        "      # sample latent vectors from the normal distribution\n",
        "      latent = mu + (z * std)\n",
        "      samples = latent.to(DEVICE)\n",
        "      fake_samples = G(class_examples_z, samples)\n",
        "      reconstructed_samples = AE.decoder(samples)\n",
        "      display_class_examples(fake_samples)\n",
        "      display_class_examples(reconstructed_samples)\n",
        "\n",
        "      if counter % EPOCHS_BETWEEN_VAL == 0:\n",
        "        ############################\n",
        "        # Validate\n",
        "        ############################\n",
        "        val_losses = GAN_epoch(testloader, AE, D, D_optimizer, G, G_optimizer,\n",
        "                                                  discrimination_loss, reconstruction_loss,\n",
        "                                                  train=False)\n",
        "\n",
        "        val_D_losses.append(losses[0])\n",
        "        val_D_x_losses.append(losses[1])\n",
        "        val_D_z_losses.append(losses[2])\n",
        "        val_D_y_losses.append(losses[3])\n",
        "        val_Dx_values.append(losses[4])\n",
        "\n",
        "        val_G_losses.append(losses[5])\n",
        "        val_G_z_losses.append(losses[6])\n",
        "        val_G_z_reconstruction_losses.append(losses[7])\n",
        "        val_G_y_losses.append(losses[8])\n",
        "        val_DGz_values.append(losses[9])\n",
        "        val_DGy_values.append(losses[10])\n",
        "\n",
        "        ############################\n",
        "        # Display\n",
        "        ############################\n",
        "        print(f\"Validation Discriminator Loss {val_D_losses[-1]:.3f} \"\n",
        "            + f\"Generator Loss {val_G_losses[-1]:.3f} \"\n",
        "            + f\"Generator Reconstruction Loss {val_G_z_reconstruction_losses[-1]:.3f} \"\n",
        "            + f\"D(x) {val_Dx_values[-1]:.3f} D(G(x)) {val_DGz_values[-1]:.3f} D(G(y)) {DGy_values[-1]:.3f}\")\n",
        "        fake_val_class_examples = G(val_class_examples_z, encoded_val_class_examples)\n",
        "        display_class_examples(val_class_examples)\n",
        "        display_class_examples(fake_val_class_examples)\n",
        "        _, latent, mu, logvar = AE(class_examples)\n",
        "        std = STD_SCALING * torch.exp(0.5*logvar)\n",
        "        z = torch.randn_like(std)\n",
        "        # sample latent vectors from the normal distribution\n",
        "        latent = mu + (z * std)\n",
        "        samples = latent.to(DEVICE)\n",
        "        fake_samples = G(class_examples_z, samples)\n",
        "        reconstructed_samples = AE.decoder(samples)\n",
        "        display_class_examples(fake_samples)\n",
        "        display_class_examples(reconstructed_samples)\n",
        "\n",
        "    D.train()\n",
        "    G.train()\n",
        "    counter += 1\n",
        "  AE.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biOHk0bKzNKd"
      },
      "source": [
        "### Visualizing GAN results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CCKf_ISC18k"
      },
      "outputs": [],
      "source": [
        "if PRE_TRAIN or ARCHITECHTURE == 'cDCGAN' or ARCHITECHTURE == 'VAE and cDCGAN':\n",
        "  plt.figure(figsize=(10,5))\n",
        "  plt.title(\"Discriminator Loss During Training\")\n",
        "  # plot Discriminator loss\n",
        "  plt.plot(D_losses,label=\"D Loss\")\n",
        "  plt.plot(D_x_losses,label=\"D x Loss\")\n",
        "  plt.plot(D_z_losses,label=\"D z Loss\")\n",
        "  # get plot axis\n",
        "  ax = plt.gca()\n",
        "  # remove right and top spine\n",
        "  ax.spines['right'].set_visible(False)\n",
        "  ax.spines['top'].set_visible(False)\n",
        "  # add labels and create legend\n",
        "  plt.xlabel(\"num_epochs\")\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwAjtn0_uRr1"
      },
      "outputs": [],
      "source": [
        "if PRE_TRAIN or ARCHITECHTURE == 'cDCGAN' or ARCHITECHTURE == 'VAE and cDCGAN':\n",
        "  plt.figure(figsize=(10,5))\n",
        "  plt.title(\"Discriminator Loss During Validation\")\n",
        "  # plot Discriminator loss\n",
        "  plt.plot(val_D_losses,label=\"Validation D Loss\")\n",
        "  plt.plot(val_D_x_losses,label=\"Validation D x Loss\")\n",
        "  plt.plot(val_D_z_losses,label=\"Validation D z Loss\")\n",
        "  # get plot axis\n",
        "  ax = plt.gca()\n",
        "  # remove right and top spine\n",
        "  ax.spines['right'].set_visible(False)\n",
        "  ax.spines['top'].set_visible(False)\n",
        "  # add labels and create legend\n",
        "  plt.xlabel(\"num_epochs\")\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HIZyYZLus1BL"
      },
      "outputs": [],
      "source": [
        "if PRE_TRAIN or ARCHITECHTURE == 'cDCGAN' or ARCHITECHTURE == 'VAE and cDCGAN':\n",
        "  plt.figure(figsize=(10,5))\n",
        "  plt.title(\"Generator Loss During Training\")\n",
        "  # plot Generator loss\n",
        "  plt.plot(G_losses,label=\"G Loss\")\n",
        "  plt.plot(G_z_losses,label=\"G z Loss\")\n",
        "  plt.plot(G_z_reconstruction_losses,label=\"G reconstruction Loss\")\n",
        "  # get plot axis\n",
        "  ax = plt.gca()\n",
        "  # remove right and top spine\n",
        "  ax.spines['right'].set_visible(False)\n",
        "  ax.spines['top'].set_visible(False)\n",
        "  # add labels and create legend\n",
        "  plt.xlabel(\"num_epochs\")\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZ8fjK-uur_D"
      },
      "outputs": [],
      "source": [
        "if PRE_TRAIN or ARCHITECHTURE == 'cDCGAN' or ARCHITECHTURE == 'VAE and cDCGAN':\n",
        "  plt.figure(figsize=(10,5))\n",
        "  plt.title(\"Generator Loss During Validation\")\n",
        "  # plot Generator loss\n",
        "  plt.plot(val_G_losses,label=\"Validation G Loss\")\n",
        "  plt.plot(val_G_z_losses,label=\"Validation G z Loss\")\n",
        "  plt.plot(val_G_z_reconstruction_losses,label=\"Validation G Reconstruction Loss\")\n",
        "  # get plot axis\n",
        "  ax = plt.gca()\n",
        "  # remove right and top spine\n",
        "  ax.spines['right'].set_visible(False)\n",
        "  ax.spines['top'].set_visible(False)\n",
        "  # add labels and create legend\n",
        "  plt.xlabel(\"num_epochs\")\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNthozIBKvPN"
      },
      "outputs": [],
      "source": [
        "if PRE_TRAIN or ARCHITECHTURE == 'cDCGAN' or ARCHITECHTURE == 'VAE and cDCGAN':\n",
        "  plt.figure(figsize=(10,5))\n",
        "  plt.title(\"Discriminator and Generator Loss During Training\")\n",
        "  # plot Discriminator and generator loss\n",
        "  plt.plot(D_losses,label=\"D Loss\")\n",
        "  plt.plot(G_losses,label=\"G Loss\")\n",
        "  # get plot axis\n",
        "  ax = plt.gca()\n",
        "  # remove right and top spine\n",
        "  ax.spines['right'].set_visible(False)\n",
        "  ax.spines['top'].set_visible(False)\n",
        "  # add labels and create legend\n",
        "  plt.xlabel(\"num_epochs\")\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgB51_LIK_2K"
      },
      "outputs": [],
      "source": [
        "if PRE_TRAIN or ARCHITECHTURE == 'cDCGAN' or ARCHITECHTURE == 'VAE and cDCGAN':\n",
        "  plt.figure(figsize=(10,5))\n",
        "  plt.title(\"Discriminator and Generator Loss During Validation\")\n",
        "  # plot Discriminator and generator loss\n",
        "  plt.plot(val_D_losses,label=\"Validation D Loss\")\n",
        "  plt.plot(val_G_losses,label=\"Validation G Loss\")\n",
        "  # get plot axis\n",
        "  ax = plt.gca()\n",
        "  # remove right and top spine\n",
        "  ax.spines['right'].set_visible(False)\n",
        "  ax.spines['top'].set_visible(False)\n",
        "  # add labels and create legend\n",
        "  plt.xlabel(\"num_epochs\")\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Xeswa2yC6xe"
      },
      "outputs": [],
      "source": [
        "if PRE_TRAIN or ARCHITECHTURE == 'cDCGAN' or ARCHITECHTURE == 'VAE and cDCGAN':\n",
        "  plt.figure(figsize=(10,5))\n",
        "  plt.title(\"Discriminator Accuracy During Training\")\n",
        "  # plot Discriminator and generator loss\n",
        "  plt.plot(Dx_values,label=\"D(x)\")\n",
        "  plt.plot(DGz_values,label=\"D(G(z))\")\n",
        "  # get plot axis\n",
        "  ax = plt.gca()\n",
        "  # remove right and top spine\n",
        "  ax.spines['right'].set_visible(False)\n",
        "  ax.spines['top'].set_visible(False)\n",
        "  # add labels and create legend\n",
        "  plt.xlabel(\"num_epochs\")\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JsGS61aEoXm"
      },
      "outputs": [],
      "source": [
        "if PRE_TRAIN or ARCHITECHTURE == 'cDCGAN' or ARCHITECHTURE == 'VAE and cDCGAN':\n",
        "  plt.figure(figsize=(10,5))\n",
        "  plt.title(\"Difference in Discriminator Accuracy During Training\")\n",
        "  # plot Discriminator and generator loss\n",
        "  diff = []\n",
        "  for i in range(len(Dx_values)):\n",
        "    diff.append(Dx_values[i] - DGz_values[i])\n",
        "  plt.plot(diff,label=\"D(x) - D(G(z))\")\n",
        "  # get plot axis\n",
        "  ax = plt.gca()\n",
        "  # remove right and top spine\n",
        "  ax.spines['right'].set_visible(False)\n",
        "  ax.spines['top'].set_visible(False)\n",
        "  # add labels and create legend\n",
        "  plt.xlabel(\"num_epochs\")\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-jkdEcWC-oy"
      },
      "outputs": [],
      "source": [
        "if PRE_TRAIN or ARCHITECHTURE == 'cDCGAN' or ARCHITECHTURE == 'VAE and cDCGAN':\n",
        "  plt.figure(figsize=(10,5))\n",
        "  plt.title(\"Discriminator Accuracy During Validation\")\n",
        "  # plot Discriminator and generator loss\n",
        "  plt.plot(val_Dx_values,label=\"Validation D(x)\")\n",
        "  plt.plot(val_DGz_values,label=\"Validation D(G(z))\")\n",
        "  # get plot axis\n",
        "  ax = plt.gca()\n",
        "  # remove right and top spine\n",
        "  ax.spines['right'].set_visible(False)\n",
        "  ax.spines['top'].set_visible(False)\n",
        "  # add labels and create legend\n",
        "  plt.xlabel(\"num_epochs\")\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKiKj-D5Jb49"
      },
      "outputs": [],
      "source": [
        "if PRE_TRAIN or ARCHITECHTURE == 'cDCGAN' or ARCHITECHTURE == 'VAE and cDCGAN':\n",
        "  plt.figure(figsize=(10,5))\n",
        "  plt.title(\"Difference in Discriminator Accuracy During Validation\")\n",
        "  # plot Discriminator and generator loss\n",
        "  diff = []\n",
        "  for i in range(len(val_Dx_values)):\n",
        "    diff.append(val_Dx_values[i] - val_DGz_values[i])\n",
        "  plt.plot(diff,label=\"Validation D(x) - D(G(z))\")\n",
        "  # get plot axis\n",
        "  ax = plt.gca()\n",
        "  # remove right and top spine\n",
        "  ax.spines['right'].set_visible(False)\n",
        "  ax.spines['top'].set_visible(False)\n",
        "  # add labels and create legend\n",
        "  plt.xlabel(\"num_epochs\")\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEcb3mcrZSDJ"
      },
      "source": [
        "### Save Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-m_k3edTdhA"
      },
      "outputs": [],
      "source": [
        "AE_path2 = \"./AE_checkpoint2\"\n",
        "torch.save({\n",
        "            'model_state_dict': AE.state_dict(),\n",
        "            'optimizer_state_dict': AE_optimizer.state_dict(),\n",
        "            }, AE_path2)\n",
        "\n",
        "G_path2 = \"./G_checkpoint2\"\n",
        "torch.save({\n",
        "            'model_state_dict': G.state_dict(),\n",
        "            'optimizer_state_dict': G_optimizer.state_dict(),\n",
        "            }, G_path2)\n",
        "\n",
        "D_path2 = \"./D_checkpoint2\"\n",
        "torch.save({\n",
        "            'model_state_dict': D.state_dict(),\n",
        "            'optimizer_state_dict': D_optimizer.state_dict(),\n",
        "            }, D_path2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQMCGsyAC783"
      },
      "source": [
        "## Train HAe/DaES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaAkn09AYUP6"
      },
      "source": [
        "### Load checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygQZET58T04N"
      },
      "outputs": [],
      "source": [
        "AE_checkpoint = torch.load(AE_path2)\n",
        "AE.load_state_dict(AE_checkpoint['model_state_dict'])\n",
        "AE_optimizer.load_state_dict(AE_checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-4IbxRmXovi"
      },
      "outputs": [],
      "source": [
        "G_checkpoint = torch.load(G_path2)\n",
        "G.load_state_dict(G_checkpoint['model_state_dict'])\n",
        "G_optimizer.load_state_dict(G_checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOzlqMugXyhR"
      },
      "outputs": [],
      "source": [
        "D_checkpoint = torch.load(D_path2)\n",
        "D.load_state_dict(D_checkpoint['model_state_dict'])\n",
        "D_optimizer.load_state_dict(D_checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fq2oirGkYZeH"
      },
      "source": [
        "###Begin Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLjdMZnkPBw-"
      },
      "outputs": [],
      "source": [
        "def HAeDaES_epoch(dataloader, AE, AE_optimizer, D, D_optimizer, G, G_optimizer,\n",
        "                 discrimination_loss, reconstruction_loss, train=True):\n",
        "  epoch_D_losses = []\n",
        "  epoch_D_x_losses = []\n",
        "  epoch_D_reconstructed_x_losses = []\n",
        "  epoch_D_y_losses = []\n",
        "  epoch_D_reconstructed_y_losses = []\n",
        "  epoch_D_z_losses = []\n",
        "  epoch_Dx = []\n",
        "\n",
        "  epoch_G_losses = []\n",
        "  epoch_G_y_losses = []\n",
        "  epoch_G_z_losses = []\n",
        "  epoch_DGz = []\n",
        "  epoch_DGy = []\n",
        "\n",
        "  epoch_AE_losses = []\n",
        "  epoch_AE_x_losses = []\n",
        "  epoch_AE_x_reconstruction_losses = []\n",
        "  epoch_AE_y_losses = []\n",
        "  epoch_DAEx = []\n",
        "  epoch_DAEy = []\n",
        "\n",
        "  step = 0\n",
        "  # iterate through data loader generator object\n",
        "  for images, classes in dataloader:\n",
        "    images = Variable(images).to(DEVICE)\n",
        "    ############################\n",
        "    # Forward Pass Through Autoencoder\n",
        "    ############################\n",
        "    reconstructed_images, encoded_images, mu, logvar = AE(images)\n",
        "\n",
        "    ############################\n",
        "    # Forward Pass Through Generator\n",
        "    ############################\n",
        "    # create noise vector z from normal distribution\n",
        "    z = torch.randn(BATCH_SIZE, size_z).to(DEVICE)\n",
        "    # generate image\n",
        "    fake_images = G(z, encoded_images)\n",
        "\n",
        "    ############################\n",
        "    # Sample From AE's Latent Space\n",
        "    ###########################\n",
        "    mean = mu.repeat(NUM_SAMPLES, 1)\n",
        "    std = STD_SCALING * torch.exp(0.5*logvar).detach().repeat(NUM_SAMPLES, 1)\n",
        "    y = torch.randn_like(std)\n",
        "\n",
        "    # sample latent vectors from the normal distribution\n",
        "    samples = mean + (y * std)\n",
        "\n",
        "    # reconstruct images from the random latent vectors\n",
        "    samples = samples.to(DEVICE)\n",
        "    reconstructed_samples = AE.decoder(samples)\n",
        "\n",
        "    ############################\n",
        "    # Forward Pass Latent Samples Through Generator\n",
        "    ############################\n",
        "    # generate image\n",
        "    fake_samples = G(z.repeat(NUM_SAMPLES, 1), samples)\n",
        "\n",
        "    ############################\n",
        "    # Calculate Discriminator loss on real images\n",
        "    ############################\n",
        "    # D_x shape will be (batch_size, ENCODED_DIM, 28, 28)\n",
        "    D_x = encoded_images.unsqueeze(2).unsqueeze(3).detach().repeat(1, 1, img_size, img_size)\n",
        "    # forward pass D(x)\n",
        "    x_preds = D(images, D_x)\n",
        "    # calculate loss log(D(x))\n",
        "    D_x_loss = discrimination_loss(x_preds, labels_real)\n",
        "\n",
        "    ############################\n",
        "    # Calculate Discriminator loss on reconstructed real images\n",
        "    ############################\n",
        "    # forward pass D(AE(x))\n",
        "    reconstructed_x_preds = D(reconstructed_images.detach(), D_x)\n",
        "    # calculate loss log(1 - D(AE(x)))\n",
        "    D_reconstructed_x_loss = discrimination_loss(reconstructed_x_preds, labels_fake)\n",
        "\n",
        "    ############################\n",
        "    # Calculate Discriminator loss on fake images\n",
        "    ############################\n",
        "    # forward pass D(G(z))\n",
        "    z_preds = D(fake_images.detach(), D_x)\n",
        "    # calculate loss log(1 - D(G(z)))\n",
        "    D_z_loss = discrimination_loss(z_preds, labels_fake)\n",
        "\n",
        "    ############################\n",
        "    # Calculate Discriminator loss on fake latent samples\n",
        "    ############################\n",
        "    # D_y shape will be (batch_size, ENCODED_DIM, 28, 28)\n",
        "    D_y = D_x.repeat(NUM_SAMPLES, 1, 1, 1)\n",
        "    # forward pass D(G(z,y))\n",
        "    y_preds = D(fake_samples.detach(), D_y)\n",
        "    # calculate loss log(1 - D(G(z,y)))\n",
        "    D_y_loss = discrimination_loss(y_preds, labels_fake.repeat(NUM_SAMPLES, 1))\n",
        "\n",
        "    ############################\n",
        "    # Calculate Discriminator loss on reconstructed latent samples\n",
        "    ############################\n",
        "    # forward pass D(AE(y))\n",
        "    reconstructed_y_preds = D(reconstructed_samples.detach(), D_y)\n",
        "    # calculate loss log(1 - D(AE(y)))\n",
        "    D_reconstructed_y_loss = discrimination_loss(reconstructed_y_preds, labels_fake.repeat(NUM_SAMPLES, 1))\n",
        "\n",
        "    ############################\n",
        "    # Update D network\n",
        "    ############################\n",
        "    D_loss = D_x_loss + (D_reconstructed_x_loss + \\\n",
        "                         D_y_loss + D_reconstructed_y_loss + \\\n",
        "                         D_z_loss)/4\n",
        "\n",
        "    # save values for plots\n",
        "    epoch_D_losses.append(D_loss.item())\n",
        "    epoch_D_x_losses.append(D_x_loss.item())\n",
        "    epoch_D_reconstructed_x_losses.append(D_reconstructed_x_loss.item()/4)\n",
        "    epoch_D_y_losses.append(D_y_loss.item()/4)\n",
        "    epoch_D_reconstructed_y_losses.append(D_reconstructed_y_loss.item()/4)\n",
        "    epoch_D_z_losses.append(D_z_loss.item()/4)\n",
        "    epoch_Dx.append(x_preds.mean().item())\n",
        "\n",
        "    if train:\n",
        "      # zero accumalted grads\n",
        "      D.zero_grad()\n",
        "      # do backward pass\n",
        "      D_loss.backward()\n",
        "      # update discriminator model\n",
        "      D_optimizer.step()\n",
        "\n",
        "    ############################\n",
        "    # Update G network and AE\n",
        "    ############################\n",
        "    # if Ksteps of Discriminator training are done, update generator\n",
        "    if step % Ksteps == 0:\n",
        "      # As we have done one step of discriminator, again calculate\n",
        "      # forward pass D(G(z))\n",
        "      z_preds = D(fake_images, D_x)\n",
        "      # calculate loss log(D(G(z)))\n",
        "      G_z_loss = discrimination_loss(z_preds, labels_real)\n",
        "\n",
        "      # forward pass D(AE(x))\n",
        "      reconstructed_x_preds = D(reconstructed_images, D_x)\n",
        "      # calculate loss log(D(AE(x)))\n",
        "      AE_x_loss = discrimination_loss(reconstructed_x_preds, labels_real)\n",
        "\n",
        "      # forward pass D(G(y))\n",
        "      y_preds = D(fake_samples, D_y)\n",
        "      # calculate loss log(D(G(y)))\n",
        "      G_y_loss = discrimination_loss(y_preds, labels_real.repeat(NUM_SAMPLES, 1))\n",
        "\n",
        "      # forward pass D(AE(y))\n",
        "      reconstructed_y_preds = D(reconstructed_samples, D_y)\n",
        "      # calculate loss log(D(AE(y)))\n",
        "      AE_y_loss = discrimination_loss(reconstructed_y_preds, labels_real.repeat(NUM_SAMPLES, 1))\n",
        "\n",
        "      # Calculate the Mean Squared Error loss between the original and reconstructed image\n",
        "      AE_x_reconstruction_loss = 100 * reconstruction_loss(reconstructed_images, images)\n",
        "      AE_x_KLD = 100 * kl_divergence(mu, logvar)\n",
        "\n",
        "      G_loss = G_y_loss + G_z_loss\n",
        "\n",
        "      AE_loss = G_loss + AE_x_loss + AE_y_loss + AE_x_reconstruction_loss + AE_x_KLD\n",
        "\n",
        "      # save values for plots\n",
        "      epoch_G_losses.append(G_loss.item())\n",
        "      epoch_G_y_losses.append(G_y_loss.item())\n",
        "      epoch_G_z_losses.append(G_z_loss.item())\n",
        "      epoch_DGz.append(z_preds.mean().item())\n",
        "      epoch_DGy.append(y_preds.mean().item())\n",
        "\n",
        "      epoch_AE_losses.append(AE_loss.item())\n",
        "      epoch_AE_x_losses.append(AE_x_loss.item())\n",
        "      epoch_AE_x_reconstruction_losses.append(AE_x_reconstruction_loss.item())\n",
        "      epoch_AE_y_losses.append(AE_y_loss.item())\n",
        "      epoch_DAEx.append(reconstructed_x_preds.mean().item())\n",
        "      epoch_DAEy.append(reconstructed_y_preds.mean().item())\n",
        "\n",
        "      if train:\n",
        "        # zero accumalted grads\n",
        "        AE.zero_grad()\n",
        "        G.zero_grad()\n",
        "        # do backward pass\n",
        "        AE_loss.backward()\n",
        "        # update generator model\n",
        "        G_optimizer.step()\n",
        "        AE_optimizer.step()\n",
        "\n",
        "    step += 1\n",
        "\n",
        "  ############################\n",
        "  # Log\n",
        "  ############################\n",
        "  epoch_D_loss = sum(epoch_D_losses)/ len(epoch_D_losses)\n",
        "  epoch_D_x_loss = sum(epoch_D_x_losses)/ len(epoch_D_x_losses)\n",
        "  epoch_D_reconstructed_x_loss = sum(epoch_D_reconstructed_x_losses)/ len(epoch_D_reconstructed_x_losses)\n",
        "  epoch_D_y_loss = sum(epoch_D_y_losses)/ len(epoch_D_y_losses)\n",
        "  epoch_D_reconstructed_y_loss = sum(epoch_D_reconstructed_y_losses)/ len(epoch_D_reconstructed_y_losses)\n",
        "  epoch_D_z_loss = sum(epoch_D_z_losses)/ len(epoch_D_z_losses)\n",
        "  epoch_Dx = sum(epoch_Dx)/ len(epoch_Dx)\n",
        "\n",
        "  epoch_G_loss = sum(epoch_G_losses)/ len(epoch_G_losses)\n",
        "  epoch_G_y_loss = sum(epoch_G_y_losses)/ len(epoch_G_y_losses)\n",
        "  epoch_G_z_loss = sum(epoch_G_z_losses)/ len(epoch_G_z_losses)\n",
        "  epoch_DGz = sum(epoch_DGz)/ len(epoch_DGz)\n",
        "  epoch_DGy = sum(epoch_DGy)/ len(epoch_DGy)\n",
        "\n",
        "  epoch_AE_loss = sum(epoch_AE_losses)/ len(epoch_AE_losses)\n",
        "  epoch_AE_x_loss = sum(epoch_AE_x_losses)/ len(epoch_AE_x_losses)\n",
        "  epoch_AE_x_reconstruction_loss = sum(epoch_AE_x_reconstruction_losses)/ len(epoch_AE_x_reconstruction_losses)\n",
        "  epoch_AE_y_loss = sum(epoch_AE_y_losses)/ len(epoch_AE_y_losses)\n",
        "  epoch_DAEx = sum(epoch_DAEx)/ len(epoch_DAEx)\n",
        "  epoch_DAEy = sum(epoch_DAEy)/ len(epoch_DAEy)\n",
        "\n",
        "  return epoch_D_loss, \\\n",
        "         epoch_D_x_loss, \\\n",
        "         epoch_D_reconstructed_x_loss, \\\n",
        "         epoch_D_y_loss, \\\n",
        "         epoch_D_reconstructed_y_loss, \\\n",
        "         epoch_D_z_loss, \\\n",
        "         epoch_Dx, \\\n",
        "         epoch_G_loss, \\\n",
        "         epoch_G_y_loss, \\\n",
        "         epoch_G_z_loss, \\\n",
        "         epoch_DGz, \\\n",
        "         epoch_DGy, \\\n",
        "         epoch_AE_loss, \\\n",
        "         epoch_AE_x_loss, \\\n",
        "         epoch_AE_x_reconstruction_loss, \\\n",
        "         epoch_AE_y_loss, \\\n",
        "         epoch_DAEx, \\\n",
        "         epoch_DAEy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IllIZmPCpU30"
      },
      "outputs": [],
      "source": [
        "if ARCHITECHTURE == \"HAe/DaES\":\n",
        "  # List of values, which will be used for plotting purpose\n",
        "  D_losses = []\n",
        "  D_x_losses = []\n",
        "  D_reconstructed_x_losses = []\n",
        "  D_y_losses = []\n",
        "  D_reconstructed_y_losses = []\n",
        "  D_z_losses = []\n",
        "  Dx_values = []\n",
        "\n",
        "  G_losses = []\n",
        "  G_y_losses = []\n",
        "  G_z_losses = []\n",
        "  DGz_values = []\n",
        "  DGy_values = []\n",
        "\n",
        "  AE_losses = []\n",
        "  AE_x_losses = []\n",
        "  AE_x_reconstruction_losses = []\n",
        "  AE_y_losses = []\n",
        "  DAEx_values = []\n",
        "  DAEy_values = []\n",
        "\n",
        "  val_D_losses = []\n",
        "  val_D_x_losses = []\n",
        "  val_D_reconstructed_x_losses = []\n",
        "  val_D_y_losses = []\n",
        "  val_D_reconstructed_y_losses = []\n",
        "  val_D_z_losses = []\n",
        "  val_Dx_values = []\n",
        "\n",
        "  val_G_losses = []\n",
        "  val_G_y_losses = []\n",
        "  val_G_z_losses = []\n",
        "  val_DGz_values = []\n",
        "  val_DGy_values = []\n",
        "\n",
        "  val_AE_losses = []\n",
        "  val_AE_x_losses = []\n",
        "  val_AE_x_reconstruction_losses = []\n",
        "  val_AE_y_losses = []\n",
        "  val_DAEx_values = []\n",
        "  val_DAEy_values = []\n",
        "\n",
        "  counter = 0\n",
        "\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    ############################\n",
        "    # Train\n",
        "    ############################\n",
        "    losses = HAeDaES_epoch(dataloader, AE, AE_optimizer, D, D_optimizer,\n",
        "                                      G, G_optimizer, discrimination_loss,\n",
        "                                      reconstruction_loss, train=True)\n",
        "    D_losses.append(losses[0])\n",
        "    D_x_losses.append(losses[1])\n",
        "    D_reconstructed_x_losses.append(losses[2])\n",
        "    D_y_losses.append(losses[3])\n",
        "    D_reconstructed_y_losses.append(losses[4])\n",
        "    D_z_losses.append(losses[5])\n",
        "    Dx_values.append(losses[6])\n",
        "\n",
        "    G_losses.append(losses[7])\n",
        "    G_y_losses.append(losses[8])\n",
        "    G_z_losses.append(losses[9])\n",
        "    DGz_values.append(losses[10])\n",
        "    DGy_values.append(losses[11])\n",
        "\n",
        "    AE_losses.append(losses[12])\n",
        "    AE_x_losses.append(losses[13])\n",
        "    AE_x_reconstruction_losses.append(losses[14])\n",
        "    AE_y_losses.append(losses[15])\n",
        "    DAEx_values.append(losses[16])\n",
        "    DAEy_values.append(losses[17])\n",
        "\n",
        "    ############################\n",
        "    # Display\n",
        "    ############################\n",
        "    AE.eval()\n",
        "    G.eval()\n",
        "    D.eval()\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} D Loss {D_losses[-1]:.3f} \"\n",
        "        + f\"G Loss {G_losses[-1]:.3f} AE Loss {AE_losses[-1]:.3f} \"\n",
        "        + f\"D(x) {Dx_values[-1]:.3f} D(G(z)) {DGz_values[-1]:.3f} \"\n",
        "        + f\"D(AE(x)) {DAEx_values[-1]:.3f} D(G(y)) {DGy_values[-1]:.3f} \"\n",
        "        + f\"D(AE(y)) {DAEy_values[-1]:.3f}\")\n",
        "    with torch.no_grad():\n",
        "      reconstructed_class_examples, encoded_class_examples, _, _ = AE(class_examples)\n",
        "      fake_class_examples = G(class_examples_z, encoded_class_examples)\n",
        "      display_class_examples(class_examples)\n",
        "      display_class_examples(fake_class_examples)\n",
        "      display_class_examples(reconstructed_class_examples)\n",
        "      _, latent, mu, logvar = AE(class_examples)\n",
        "      std = STD_SCALING * torch.exp(0.5*logvar)\n",
        "      z = torch.randn_like(std)\n",
        "      # sample latent vectors from the normal distribution\n",
        "      latent = mu + (z * std)\n",
        "      samples = latent.to(DEVICE)\n",
        "      fake_samples = G(class_examples_z, samples)\n",
        "      reconstructed_samples = AE.decoder(samples)\n",
        "      display_class_examples(fake_samples)\n",
        "      display_class_examples(reconstructed_samples)\n",
        "\n",
        "\n",
        "      if counter % EPOCHS_BETWEEN_VAL == 0:\n",
        "        ############################\n",
        "        # Validate\n",
        "        ############################\n",
        "        val_losses = HAeDaES_epoch(testloader, AE, AE_optimizer, D, D_optimizer,\n",
        "                                   G, G_optimizer, discrimination_loss,\n",
        "                                   reconstruction_loss, train=False)\n",
        "        val_D_losses.append(val_losses[0])\n",
        "        val_D_x_losses.append(val_losses[1])\n",
        "        val_D_reconstructed_x_losses.append(val_losses[2])\n",
        "        val_D_y_losses.append(val_losses[3])\n",
        "        val_D_reconstructed_y_losses.append(val_losses[4])\n",
        "        val_D_z_losses.append(val_losses[5])\n",
        "        val_Dx_values.append(val_losses[6])\n",
        "\n",
        "        val_G_losses.append(val_losses[7])\n",
        "        val_G_y_losses.append(val_losses[8])\n",
        "        val_G_z_losses.append(val_losses[9])\n",
        "        val_DGz_values.append(val_losses[10])\n",
        "        val_DGy_values.append(val_losses[11])\n",
        "\n",
        "        val_AE_losses.append(val_losses[12])\n",
        "        val_AE_x_losses.append(val_losses[13])\n",
        "        val_AE_x_reconstruction_losses.append(val_losses[14])\n",
        "        val_AE_y_losses.append(val_losses[15])\n",
        "        val_DAEx_values.append(val_losses[16])\n",
        "        val_DAEy_values.append(val_losses[17])\n",
        "\n",
        "        ############################\n",
        "        # Display\n",
        "        ############################\n",
        "        print(f\"Validation D Loss {val_D_losses[-1]:.3f} \"\n",
        "            + f\"G Loss {val_G_losses[-1]:.3f} AE Loss {val_AE_losses[-1]:.3f} \"\n",
        "            + f\"D(x) {val_Dx_values[-1]:.3f} D(G(z)) {val_DGz_values[-1]:.3f} \"\n",
        "            + f\"D(AE(x)) {val_DAEx_values[-1]:.3f} D(G(y)) {val_DGy_values[-1]:.3f} \"\n",
        "            + f\"D(AE(y)) {val_DAEy_values[-1]:.3f}\")\n",
        "        reconstructed_val_class_examples, encoded_val_class_examples, _, _ = AE(val_class_examples)\n",
        "        fake_val_class_examples = G(val_class_examples_z, encoded_val_class_examples)\n",
        "        reconstructed_fake_val_class_examples, _, _, _ = AE(fake_val_class_examples)\n",
        "        display_class_examples(val_class_examples)\n",
        "        display_class_examples(fake_val_class_examples)\n",
        "        display_class_examples(reconstructed_val_class_examples)\n",
        "        _, latent, mu, logvar = AE(val_class_examples)\n",
        "        std = STD_SCALING * torch.exp(0.5*logvar)\n",
        "        z = torch.randn_like(std)\n",
        "        # sample latent vectors from the normal distribution\n",
        "        latent = mu + (z * std)\n",
        "        samples = latent.to(DEVICE)\n",
        "        fake_samples = G(val_class_examples_z, samples)\n",
        "        reconstructed_samples = AE.decoder(samples)\n",
        "        display_class_examples(fake_samples)\n",
        "        display_class_examples(reconstructed_samples)\n",
        "    D.train()\n",
        "    G.train()\n",
        "    AE.train()\n",
        "    counter += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJ2Knh9YcoyC"
      },
      "source": [
        "### Visualizing HAe/DaES Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hr1WJTuzcoyE"
      },
      "outputs": [],
      "source": [
        "def plot_losses(training_losses, validation_losses, offset_factor):\n",
        "    x_vals = [i * offset_factor for i in range(len(validation_losses))]\n",
        "    plt.plot(x_vals, validation_losses, label='Validation Loss')\n",
        "    plt.plot(range(len(training_losses)), training_losses,\n",
        "             label='Training Loss')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "plot_losses(AE_losses, val_AE_losses, EPOCHS_BETWEEN_VAL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dQWZHhxiK-N"
      },
      "outputs": [],
      "source": [
        "for i in range(len(D_reconstructed_x_losses)):\n",
        "  D_reconstructed_x_losses[i] = D_reconstructed_x_losses[i]/4\n",
        "  D_reconstructed_y_losses[i] = D_reconstructed_y_losses[i]/4\n",
        "  D_y_losses[i] = D_y_losses[i]/4\n",
        "  D_z_losses[i] = D_z_losses[i]/4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JlTgv-xA1Qj"
      },
      "outputs": [],
      "source": [
        "if PRE_TRAIN or ARCHITECHTURE == 'cDCGAN' or ARCHITECHTURE == 'VAE and cDCGAN':\n",
        "  plt.figure(figsize=(10,5))\n",
        "  plt.title(\"Discriminator Loss During Training\")\n",
        "  # plot Discriminator loss\n",
        "  plt.plot(D_losses,label=\"Discriminator Loss\")\n",
        "  plt.plot(D_x_losses,label=\"Discriminator x Loss\")\n",
        "  plt.plot(D_reconstructed_x_losses,label=\"Discriminator AE(x) Loss\")\n",
        "  plt.plot(D_y_losses,label=\"Discriminator G(y) Loss\")\n",
        "  plt.plot(D_reconstructed_y_losses,label=\"Discriminator AE(y) Loss\")\n",
        "  plt.plot(D_z_losses,label=\"Discriminator G(z) Loss\")\n",
        "  # get plot axis\n",
        "  ax = plt.gca()\n",
        "  # remove right and top spine\n",
        "  ax.spines['right'].set_visible(False)\n",
        "  ax.spines['top'].set_visible(False)\n",
        "  # add labels and create legend\n",
        "  plt.xlabel(\"num_epochs\")\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9NlCoeyAMkS"
      },
      "outputs": [],
      "source": [
        "if PRE_TRAIN or ARCHITECHTURE == 'cDCGAN' or ARCHITECHTURE == 'VAE and cDCGAN':\n",
        "  plt.figure(figsize=(10,5))\n",
        "  plt.title(\"Discriminator Loss During Training\")\n",
        "  # plot Discriminator loss\n",
        "  plt.plot(DGy_values, label=\"Discriminator Loss\")\n",
        "  plt.plot(D_y_losses, label=\"Discriminator Loss\")\n",
        "  # get plot axis\n",
        "  ax = plt.gca()\n",
        "  # remove right and top spine\n",
        "  ax.spines['right'].set_visible(False)\n",
        "  ax.spines['top'].set_visible(False)\n",
        "  # add labels and create legend\n",
        "  plt.xlabel(\"num_epochs\")\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGtVVQHrc75P"
      },
      "outputs": [],
      "source": [
        "if PRE_TRAIN or ARCHITECHTURE == 'cDCGAN' or ARCHITECHTURE == 'VAE and cDCGAN':\n",
        "  plt.figure(figsize=(10,5))\n",
        "  plt.title(\"Autoencoder Loss During Training\")\n",
        "  # plot Discriminator loss\n",
        "  # plt.plot(AE_losses,label=\"AE Loss\")\n",
        "  # plt.plot(AE_x_losses,label=\"AE x Loss\")\n",
        "  # plt.plot(AE_x_reconstruction_losses,label=\"AE x Reconstruction Loss\")\n",
        "  # plt.plot(AE_y_losses,label=\"AE y Loss\")\n",
        "  # plt.plot(AE_y_reconstruction_losses,label=\"AE y Reconstruction Loss\")\n",
        "  plt.plot(G_y_losses,label=\"G y Loss\")\n",
        "  plt.plot(G_z_losses,label=\"G z Loss\")\n",
        "  # get plot axis\n",
        "  ax = plt.gca()\n",
        "  # remove right and top spine\n",
        "  ax.spines['right'].set_visible(False)\n",
        "  ax.spines['top'].set_visible(False)\n",
        "  # add labels and create legend\n",
        "  plt.xlabel(\"num_epochs\")\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xw1HNksPetjJ"
      },
      "outputs": [],
      "source": [
        "images, _ = next(iter(dataloader))\n",
        "images = Variable(images).to(DEVICE)\n",
        "reconstructed_images, encoded_images, _, _ = AE(images)\n",
        "D_x = encoded_images.unsqueeze(2).unsqueeze(3).detach().repeat(1, 1, img_size, img_size)\n",
        "x_preds = D(images, D_x)\n",
        "reconstructed_x_preds = D(reconstructed_images.detach(), D_x)\n",
        "print(torch.max(x_preds))\n",
        "print(torch.max(reconstructed_x_preds))\n",
        "print(torch.min(x_preds))\n",
        "print(torch.min(reconstructed_x_preds))\n",
        "print(discrimination_loss(x_preds, labels_real))\n",
        "print(discrimination_loss(x_preds, labels_fake))\n",
        "print(discrimination_loss(reconstructed_x_preds, labels_real))\n",
        "print(discrimination_loss(reconstructed_x_preds, labels_fake))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPWgackMcoyG"
      },
      "outputs": [],
      "source": [
        "def sort_data(data, labels):\n",
        "  \"\"\"\n",
        "  Sorts the data and labels tensors by label.\n",
        "  Args:\n",
        "    data: a tensor of data samples.\n",
        "    labels: a tensor of corresponding labels.\n",
        "  Returns:\n",
        "    A list of sorted data tensors, where the i-th tensor in the list contains all data samples with label i.\n",
        "  \"\"\"\n",
        "  # Create a list of 10 empty tensors to store the sorted data\n",
        "  sorted_data = [torch.empty((0, data.shape[1])).cuda() for _ in range(10)]\n",
        "\n",
        "  # Iterate through the data and labels tensors\n",
        "  for d, l in zip(data, labels):\n",
        "    # Append the data entry to the appropriate tensor in the sorted_data list\n",
        "    sorted_data[l] = torch.cat((sorted_data[l], d.unsqueeze(0)))\n",
        "\n",
        "  return sorted_data\n",
        "\n",
        "if PRE_TRAIN or ARCHITECHTURE == 'VAE' or ARCHITECHTURE == 'VAE and cDCGAN':\n",
        "  encoded_class_sums = [torch.zeros(ENCODED_DIM).cuda() for _ in range(10)]\n",
        "  class_totals = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "  for img, classes in dataloader:\n",
        "    images = Variable(img).cuda()\n",
        "    labels = Variable(classes).cuda()\n",
        "    with torch.no_grad():\n",
        "      _, encoded_images, _, _ = AE(images)\n",
        "    # Sort the encoded images by label\n",
        "    sorted_encoded_images = sort_data(encoded_images, labels)\n",
        "    # Iterate through the sorted encoded image tensors\n",
        "    for i in range(len(sorted_encoded_images)):\n",
        "      # Add the sum of the encoded images in the current tensor to the encoded class sum for this label\n",
        "      encoded_class_sums[i] = torch.add(torch.sum(sorted_encoded_images[i], 0),\n",
        "                                        encoded_class_sums[i])\n",
        "\n",
        "      # Increment the class total for this label by the number of encoded images in the current tensor\n",
        "      class_totals[i] += len(sorted_encoded_images[i])\n",
        "\n",
        "  # Initialize a list to store the class encodings\n",
        "  class_encodings = [torch.zeros(ENCODED_DIM) for _ in range(10)]\n",
        "\n",
        "  # Iterate through the encoded class sums\n",
        "  for l in range(len(encoded_class_sums)):\n",
        "    # Calculate the class encoding as the mean of the encoded images for each class\n",
        "    class_encodings[l] = torch.div(encoded_class_sums[l], class_totals[l]).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edGOWe16coyJ"
      },
      "outputs": [],
      "source": [
        "if PRE_TRAIN or ARCHITECHTURE == 'VAE' or ARCHITECHTURE == 'VAE and cDCGAN':\n",
        "  tensors = []\n",
        "  for i in range(len(class_encodings)):\n",
        "    with torch.no_grad():\n",
        "      tensors.append(AE.decoder(class_encodings[i]).detach())\n",
        "  # Reshape the tensors to [28, 28]\n",
        "  images = [np.squeeze(tensor).cpu() for tensor in tensors]\n",
        "\n",
        "  # Create a figure with a grid of subplots\n",
        "  fig, axes = plt.subplots(nrows=1, ncols=10, figsize=(20, 2.5))\n",
        "\n",
        "  # Flatten the axes array\n",
        "  axes = axes.flatten()\n",
        "\n",
        "  # Iterate over the images and add them to the subplots\n",
        "  for image, ax in zip(images, axes):\n",
        "      ax.imshow(image, cmap='gray')\n",
        "      ax.axis('off')\n",
        "\n",
        "  # Show the plot\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Mx-e-bEcoyL"
      },
      "outputs": [],
      "source": [
        "if PRE_TRAIN or ARCHITECHTURE == 'VAE' or ARCHITECHTURE == 'VAE and cDCGAN':\n",
        "  image_features = torch.stack(class_encodings, dim=0).squeeze()\n",
        "  image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "  similarity = image_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
        "  plt.figure(figsize=(20, 14))\n",
        "  plt.imshow(similarity)\n",
        "  plt.yticks(range(10), fontsize=18)\n",
        "  plt.xticks(range(10), fontsize=18)\n",
        "  for x in range(similarity.shape[1]):\n",
        "      for y in range(similarity.shape[0]):\n",
        "          plt.text(x, y, f\"{similarity[y, x]:.2f}\", ha=\"center\", va=\"center\",\n",
        "                   size=12)\n",
        "\n",
        "  for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
        "    plt.gca().spines[side].set_visible(False)\n",
        "\n",
        "  plt.xlim([-0.5, 10 - 0.5])\n",
        "  plt.ylim([9 + 0.5, -2])\n",
        "\n",
        "  plt.title(\"Cosine similarity between averaged image features\", size=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SITYKm5jcoyN"
      },
      "outputs": [],
      "source": [
        "AE.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    # calculate mean and std of latent code, generated takining in test images as inputs\n",
        "    images, labels = next(iter(dataloader))\n",
        "    images = images.to(DEVICE)\n",
        "    _, latent, mu, logvar = AE(images)\n",
        "    latent = latent.cpu()\n",
        "\n",
        "    std = 10 * torch.exp(0.5*logvar)\n",
        "\n",
        "    z = torch.randn_like(std)\n",
        "\n",
        "    # sample latent vectors from the normal distribution\n",
        "    latent = mu + (z * std)\n",
        "\n",
        "    # reconstruct images from the random latent vectors\n",
        "    latent = latent.to(DEVICE)\n",
        "    img_recon = AE.decoder(latent)\n",
        "    img_recon = img_recon.cpu()\n",
        "\n",
        "    z = torch.randn(len(latent), size_z).to(DEVICE)\n",
        "    img_fake = G(z, latent).cpu()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(20, 8.5))\n",
        "    show_image(torchvision.utils.make_grid(img_fake[:100],10,5))\n",
        "    plt.show()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(20, 8.5))\n",
        "    show_image(torchvision.utils.make_grid(img_recon[:100],10,5))\n",
        "    plt.show()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(20, 8.5))\n",
        "    show_image(torchvision.utils.make_grid(images[:100].cpu(),10,5))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0P6_JM5Imx3"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    # calculate mean and std of latent code, generated takining in test images as inputs\n",
        "    images, labels = next(iter(dataloader))\n",
        "    images = images.to(DEVICE)\n",
        "    _, latent, _, _ = AE(images)\n",
        "    latent = latent.cpu()\n",
        "\n",
        "    mean = latent.mean(dim=0)\n",
        "    std = (latent - mean).pow(2).mean(dim=0).sqrt()\n",
        "\n",
        "    # sample latent vectors from the normal distribution\n",
        "    latent = torch.randn(128, ENCODED_DIM)*std + mean\n",
        "\n",
        "    # reconstruct images from the random latent vectors\n",
        "    latent = latent.to(DEVICE)\n",
        "    img_recon = AE.decoder(latent)\n",
        "    img_recon = img_recon.cpu()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(20, 8.5))\n",
        "    show_image(torchvision.utils.make_grid(img_recon[:100],10,5))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-r90v84tI01j"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    # calculate mean and std of latent code, generated takining in test images as inputs\n",
        "    images, labels = next(iter(testloader))\n",
        "    images = images.to(DEVICE)\n",
        "    _, latent, mu, log_var = AE(images)\n",
        "    latent = latent.cpu()\n",
        "\n",
        "    mean = mu.mean(dim=0).cpu()\n",
        "    std = STD_SCALING * torch.exp(0.5*log_var).mean(dim=0).cpu()\n",
        "\n",
        "    # sample latent vectors from the normal distribution\n",
        "    latent = torch.randn(128, ENCODED_DIM)*std + mean\n",
        "\n",
        "    # reconstruct images from the random latent vectors\n",
        "    latent = latent.to(DEVICE)\n",
        "    img_recon = AE.decoder(latent)\n",
        "    img_recon = img_recon.cpu()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(20, 8.5))\n",
        "    show_image(torchvision.utils.make_grid(img_recon[:100],10,5))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKEEH2Dp8ilW"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    # calculate mean and std of latent code, generated takining in test images as inputs\n",
        "    images, labels = next(iter(testloader))\n",
        "    images = images.to(DEVICE)\n",
        "    _, latent, mu, log_var = AE(images)\n",
        "    latent = latent.cpu()\n",
        "\n",
        "    mean = mu.mean(dim=0).cpu()\n",
        "    std = STD_SCALING * torch.exp(0.5*log_var).mean(dim=0).cpu()\n",
        "\n",
        "    # sample latent vectors from the normal distribution\n",
        "    latent = torch.randn(128, ENCODED_DIM)*std + mean\n",
        "\n",
        "    # reconstruct images from the random latent vectors\n",
        "    z = torch.randn(len(latent), size_z).to(DEVICE)\n",
        "    latent = latent.to(DEVICE)\n",
        "    img_recon = G(z, latent)\n",
        "    img_recon = img_recon.cpu()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(20, 8.5))\n",
        "    show_image(torchvision.utils.make_grid(img_recon[:100],10,5))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__HLVtqkOGK9"
      },
      "source": [
        "### Save Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxdGfUyHODwI"
      },
      "outputs": [],
      "source": [
        "AE_path3 = \"./AE_checkpoint3\"\n",
        "torch.save({\n",
        "            'model_state_dict': AE.state_dict(),\n",
        "            'optimizer_state_dict': AE_optimizer.state_dict(),\n",
        "            }, AE_path3)\n",
        "\n",
        "G_path3 = \"./G_checkpoint3\"\n",
        "torch.save({\n",
        "            'model_state_dict': G.state_dict(),\n",
        "            'optimizer_state_dict': G_optimizer.state_dict(),\n",
        "            }, G_path3)\n",
        "\n",
        "D_path3 = \"./D_checkpoint3\"\n",
        "torch.save({\n",
        "            'model_state_dict': D.state_dict(),\n",
        "            'optimizer_state_dict': D_optimizer.state_dict(),\n",
        "            }, D_path3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a34F4KuLOWGg"
      },
      "source": [
        "### Load checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_-zyA2TOWGh"
      },
      "outputs": [],
      "source": [
        "AE_checkpoint = torch.load(AE_path3)\n",
        "AE.load_state_dict(AE_checkpoint['model_state_dict'])\n",
        "AE_optimizer.load_state_dict(AE_checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkdR769aOWGh"
      },
      "outputs": [],
      "source": [
        "G_checkpoint = torch.load(G_path3)\n",
        "G.load_state_dict(G_checkpoint['model_state_dict'])\n",
        "G_optimizer.load_state_dict(G_checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Q1rnPESOWGh"
      },
      "outputs": [],
      "source": [
        "D_checkpoint = torch.load(D_path3)\n",
        "D.load_state_dict(D_checkpoint['model_state_dict'])\n",
        "D_optimizer.load_state_dict(D_checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FidiyUuGoNJa"
      },
      "source": [
        "# Visualizing the Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkKQ7dVgNCaZ"
      },
      "outputs": [],
      "source": [
        "results, _, _, _ = AE(img.to(DEVICE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3DWwTELNZkR"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Input tensor of shape [32, 1, 28, 28]\n",
        "tensor = img.cpu()\n",
        "print(tensor.shape)\n",
        "\n",
        "# Reshape the tensor to [32, 28, 28]\n",
        "tensor = tensor.reshape(-1, 28, 28)\n",
        "\n",
        "# Create a figure with 8 rows and 4 columns\n",
        "fig, axes = plt.subplots(8, 4, figsize=(20, 20))\n",
        "axes = axes.ravel()\n",
        "\n",
        "# Plot each image in the grid\n",
        "for i in range(32):\n",
        "    axes[i].imshow(tensor[i], cmap='gray')\n",
        "    axes[i].axis('off')\n",
        "\n",
        "# Show the grid of images\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mkSzJBQNdOK"
      },
      "outputs": [],
      "source": [
        "# Input tensor of shape [32, 1, 28, 28]\n",
        "tensor = results.detach().cpu()\n",
        "\n",
        "# Reshape the tensor to [32, 28, 28]\n",
        "tensor = tensor.reshape(-1, 28, 28)\n",
        "\n",
        "# Create a figure with 8 rows and 4 columns\n",
        "fig, axes = plt.subplots(8, 4, figsize=(20, 20))\n",
        "axes = axes.ravel()\n",
        "\n",
        "# Plot each image in the grid\n",
        "for i in range(32):\n",
        "    axes[i].imshow(tensor[i], cmap='gray')\n",
        "    axes[i].axis('off')\n",
        "\n",
        "# Show the grid of images\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0za68sQaPGQr"
      },
      "outputs": [],
      "source": [
        "if PRE_TRAIN or ARCHITECHTURE == 'VAE' or ARCHITECHTURE == 'VAE and cDCGAN':\n",
        "  encoded_class_sums = [torch.zeros(ENCODED_DIM).cuda() for _ in range(10)]\n",
        "  class_totals = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
        "  for img, classes in dataloader:\n",
        "    images = Variable(img).cuda()\n",
        "    labels = Variable(classes).cuda()\n",
        "    with torch.no_grad():\n",
        "      _, encoded_images, _, _ = AE(images)\n",
        "    # Sort the encoded images by label\n",
        "    sorted_encoded_images = sort_data(encoded_images, labels)\n",
        "    # Iterate through the sorted encoded image tensors\n",
        "    for i in range(len(sorted_encoded_images)):\n",
        "      # Add the sum of the encoded images in the current tensor to the encoded class sum for this label\n",
        "      encoded_class_sums[i] = torch.add(torch.sum(sorted_encoded_images[i], 0),\n",
        "                                        encoded_class_sums[i])\n",
        "\n",
        "      # Increment the class total for this label by the number of encoded images in the current tensor\n",
        "      class_totals[i] += len(sorted_encoded_images[i])\n",
        "\n",
        "  # Initialize a list to store the class encodings\n",
        "  class_encodings = [torch.zeros(ENCODED_DIM) for _ in range(10)]\n",
        "\n",
        "  # Iterate through the encoded class sums\n",
        "  for l in range(len(encoded_class_sums)):\n",
        "    # Calculate the class encoding as the mean of the encoded images for each class\n",
        "    class_encodings[l] = torch.div(encoded_class_sums[l], class_totals[l]).unsqueeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBblGd6jPQqj"
      },
      "outputs": [],
      "source": [
        "if PRE_TRAIN or ARCHITECHTURE == 'VAE' or ARCHITECHTURE == 'VAE and cDCGAN':\n",
        "  tensors = []\n",
        "  for i in range(len(class_encodings)):\n",
        "    with torch.no_grad():\n",
        "      tensors.append(AE.decoder(class_encodings[i]).detach())\n",
        "  # Reshape the tensors to [28, 28]\n",
        "  images = [np.squeeze(tensor).cpu() for tensor in tensors]\n",
        "\n",
        "  # Create a figure with a grid of subplots\n",
        "  fig, axes = plt.subplots(nrows=1, ncols=10, figsize=(20, 2.5))\n",
        "\n",
        "  # Flatten the axes array\n",
        "  axes = axes.flatten()\n",
        "\n",
        "  # Iterate over the images and add them to the subplots\n",
        "  for image, ax in zip(images, axes):\n",
        "      ax.imshow(image, cmap='gray')\n",
        "      ax.axis('off')\n",
        "\n",
        "  # Show the plot\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2-NIhCnPbtE"
      },
      "outputs": [],
      "source": [
        "image_features = torch.stack(class_encodings, dim=0).squeeze()\n",
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "similarity = image_features.cpu().numpy() @ image_features.cpu().numpy().T\n",
        "plt.figure(figsize=(20, 14))\n",
        "plt.imshow(similarity)\n",
        "plt.yticks(range(10), fontsize=18)\n",
        "plt.xticks(range(10), fontsize=18)\n",
        "for x in range(similarity.shape[1]):\n",
        "    for y in range(similarity.shape[0]):\n",
        "        plt.text(x, y, f\"{similarity[y, x]:.2f}\", ha=\"center\", va=\"center\", size=12)\n",
        "\n",
        "for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
        "  plt.gca().spines[side].set_visible(False)\n",
        "\n",
        "plt.xlim([-0.5, 10 - 0.5])\n",
        "plt.ylim([9 + 0.5, -2])\n",
        "\n",
        "plt.title(\"Cosine similarity between averaged image features\", size=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eaDCMd-_Ik95"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    # calculate mean and std of latent code, generated takining in test images as inputs\n",
        "    images, labels = next(iter(dataloader))\n",
        "    images = images.to(DEVICE)\n",
        "    _, latent, _, _ = AE(images)\n",
        "    latent = latent.cpu()\n",
        "\n",
        "    mean = latent.mean(dim=0)\n",
        "    std = (latent - mean).pow(2).mean(dim=0).sqrt()\n",
        "\n",
        "    # sample latent vectors from the normal distribution\n",
        "    latent = torch.randn(128, ENCODED_DIM)*std + mean\n",
        "\n",
        "    # reconstruct images from the random latent vectors\n",
        "    latent = latent.to(DEVICE)\n",
        "    img_recon = AE.decoder(latent)\n",
        "    img_recon = img_recon.cpu()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(20, 8.5))\n",
        "    show_image(torchvision.utils.make_grid(img_recon[:100],10,5))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD0jTPLKZzTu"
      },
      "source": [
        "# Visualizing the GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urOunOeud4a5"
      },
      "source": [
        "\n",
        "\n",
        "### Plot for Discriminator and Generator loss over the epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCMwKVoT4x5A"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Discriminator and Generator loss during Training\")\n",
        "# plot Discriminator and generator loss\n",
        "plt.plot(D_losses,label=\"D Loss\")\n",
        "plt.plot(G_losses,label=\"G Loss\")\n",
        "# get plot axis\n",
        "ax = plt.gca()\n",
        "# remove right and top spine\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.spines['top'].set_visible(False)\n",
        "# add labels and create legend\n",
        "plt.xlabel(\"num_epochs\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlpOhp5P24Yx"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Discriminator accuracy during Training\")\n",
        "# plot Discriminator and generator loss\n",
        "plt.plot(Dx_values,label=\"Dx\")\n",
        "plt.plot(DGz_values,label=\"DGz\")\n",
        "plt.plot(DAEy_values,label=\"DAEy\")\n",
        "plt.plot(DAEx_values,label=\"DAEx\")\n",
        "plt.plot(DGy_values,label=\"DGy\")\n",
        "# get plot axis\n",
        "ax = plt.gca()\n",
        "# remove right and top spine\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.spines['top'].set_visible(False)\n",
        "# add labels and create legend\n",
        "plt.xlabel(\"num_epochs\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4a23RyCBws1"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"Discriminator accuracy during testing\")\n",
        "# plot Discriminator and generator loss\n",
        "plt.plot(val_Dx_values,label=\"Val D Loss\")\n",
        "plt.plot(val_D_losses,label=\"Val D(x)\")\n",
        "# get plot axis\n",
        "ax = plt.gca()\n",
        "# remove right and top spine\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.spines['top'].set_visible(False)\n",
        "# add labels and create legend\n",
        "plt.xlabel(\"num_epochs\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QB2NsRTKcew"
      },
      "outputs": [],
      "source": [
        "images, labels = next(iter(dataloader))\n",
        "tensor1 = class_examples[1]\n",
        "tensor2 = class_examples[2]\n",
        "n=20\n",
        "interpolated_tensors = []\n",
        "for i in range(n):\n",
        "    alpha = i / (n - 1)\n",
        "    interpolated_tensor = (1 - alpha) * tensor1 + alpha * tensor2\n",
        "    interpolated_tensors.append(interpolated_tensor)\n",
        "\n",
        "# Convert the list of tensors to a single tensor\n",
        "interpolated_tensor = torch.stack(interpolated_tensors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2fEenjVLTAo"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "  for i in range(10):\n",
        "    for j in range(10):\n",
        "      tensor1 = class_examples[i]\n",
        "      tensor2 = class_examples[j]\n",
        "      n=20\n",
        "      interpolated_tensors = []\n",
        "      for k in range(n):\n",
        "          alpha = k / (n - 1)\n",
        "          interpolated_tensor = (1 - alpha) * tensor1 + alpha * tensor2\n",
        "          interpolated_tensors.append(interpolated_tensor)\n",
        "\n",
        "      # Convert the list of tensors to a single tensor\n",
        "      interpolated_tensor = torch.stack(interpolated_tensors)\n",
        "      reconstructed, encoded, _, _ = AE(interpolated_tensor)\n",
        "      # Reshape the tensors to [28, 28]\n",
        "      images = [np.squeeze(tensor, axis=0).cpu() for tensor in reconstructed]\n",
        "\n",
        "      # Create a figure with a grid of subplots\n",
        "      fig, axes = plt.subplots(nrows=1, ncols=20, figsize=(20, 2.5))\n",
        "\n",
        "      # Flatten the axes array\n",
        "      axes = axes.flatten()\n",
        "\n",
        "      # Iterate over the images and add them to the subplots\n",
        "      for image, ax in zip(images, axes):\n",
        "        ax.imshow(image, cmap='gray')\n",
        "        ax.axis('off')\n",
        "      # Show the plot\n",
        "      plt.show()\n",
        "\n",
        "      reconstructed, encoded, _, _ = AE(interpolated_tensor)\n",
        "      z = torch.randn(len(encoded), size_z).to(DEVICE)\n",
        "      fake = G(z, encoded)\n",
        "      # Reshape the tensors to [28, 28]\n",
        "      images = [np.squeeze(tensor, axis=0).cpu() for tensor in fake]\n",
        "\n",
        "      # Create a figure with a grid of subplots\n",
        "      fig, axes = plt.subplots(nrows=1, ncols=20, figsize=(20, 2.5))\n",
        "\n",
        "      # Flatten the axes array\n",
        "      axes = axes.flatten()\n",
        "\n",
        "      # Iterate over the images and add them to the subplots\n",
        "      for image, ax in zip(images, axes):\n",
        "        ax.imshow(image, cmap='gray')\n",
        "        ax.axis('off')\n",
        "\n",
        "      # Show the plot\n",
        "      plt.show()\n",
        "      print(\"------------------------------------------------------------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nf9MPo1Lu1A"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "  reconstructed, encoded, _, _ = AE(interpolated_tensor)\n",
        "  z = torch.randn(len(encoded), size_z).to(DEVICE)\n",
        "  fake = G(z, encoded)\n",
        "  # Reshape the tensors to [28, 28]\n",
        "  images = [np.squeeze(tensor, axis=0).cpu() for tensor in fake]\n",
        "\n",
        "  # Create a figure with a grid of subplots\n",
        "  fig, axes = plt.subplots(nrows=1, ncols=20, figsize=(20, 2.5))\n",
        "\n",
        "  # Flatten the axes array\n",
        "  axes = axes.flatten()\n",
        "\n",
        "  # Iterate over the images and add them to the subplots\n",
        "  for image, ax in zip(images, axes):\n",
        "    ax.imshow(image, cmap='gray')\n",
        "    ax.axis('off')\n",
        "\n",
        "  # Show the plot\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSDo9Fmhz7zA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def save_images(images, folder_path, batch_idx):\n",
        "    # Create folder if it doesn't exist\n",
        "    os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "    # Loop through images in batch\n",
        "    for i, image in enumerate(images.cpu()):\n",
        "        # Convert numpy array to PIL image\n",
        "        pil_image = Image.fromarray(np.uint8(image[0] * 255))\n",
        "\n",
        "        # Generate unique file name based on batch index and image index\n",
        "        file_name = f\"{batch_idx * images.shape[0] + i}.png\"\n",
        "\n",
        "        # Save image to folder\n",
        "        pil_image.save(os.path.join(folder_path, file_name))\n",
        "\n",
        "with torch.no_grad():\n",
        "  i=0\n",
        "  for images, classes in testloader:\n",
        "    images = Variable(images).to(DEVICE)\n",
        "    save_images(images, \"./original\", i)\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gC7zZMIjkurm"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "  i=0\n",
        "  for images, classes in testloader:\n",
        "    images = Variable(images).to(DEVICE)\n",
        "    reconstruction, encoded, _, _ = AE(images)\n",
        "    save_images(reconstruction, \"./reconstructed\", i)\n",
        "\n",
        "    z = torch.randn(BATCH_SIZE, size_z).to(DEVICE)\n",
        "    # generate image\n",
        "    fake_images = G(z, encoded_images)\n",
        "    save_images(fake_images, \"./generated\", i)\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxG0_P-n3CTZ"
      },
      "outputs": [],
      "source": [
        "! pip install pytorch-fid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qi7ejd2Q3IBD"
      },
      "outputs": [],
      "source": [
        "! python -m pytorch_fid ./original ./reconstructed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFgQFSqH3taY"
      },
      "outputs": [],
      "source": [
        "! python -m pytorch_fid ./original ./generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dkw4WQX5466k"
      },
      "outputs": [],
      "source": [
        "AE_test = Autoencoder(ENCODED_DIM).to(DEVICE)\n",
        "print(AE_test)\n",
        "AE_total_params = sum(p.numel() for p in AE_test.parameters() if p.requires_grad)\n",
        "print(f\"Total Trainable Parameters: {AE_total_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5QRf4_w5W7s"
      },
      "outputs": [],
      "source": [
        "AE_test_optimizer = torch.optim.Adam(AE_test.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUTqVALt5KOH"
      },
      "outputs": [],
      "source": [
        "AE_test_losses = []\n",
        "test_reconstruction_losses = []\n",
        "\n",
        "val_AE_test_losses = []\n",
        "val_test_reconstruction_losses = []\n",
        "\n",
        "counter = 0\n",
        "for epoch in range(NUM_EPOCHS + NUM_PRETRAIN_EPOCHS):\n",
        "  ############################\n",
        "  # Train\n",
        "  ############################\n",
        "  losses = AE_epoch(dataloader, AE_test, AE_test_optimizer,\n",
        "                    reconstruction_loss, train=True)\n",
        "\n",
        "  AE_test_losses.append(losses[0])\n",
        "  test_reconstruction_losses.append(losses[1])\n",
        "\n",
        "  ############################\n",
        "  # Display\n",
        "  ############################\n",
        "  AE_test.eval()\n",
        "  print('epoch [{}/{}], loss:{:.3f}, reconstruction:{:.3f}'.format(epoch+1, NUM_EPOCHS + NUM_PRETRAIN_EPOCHS,\n",
        "                                                        AE_test_losses[-1],\n",
        "                                                        test_reconstruction_losses[-1]))\n",
        "  with torch.no_grad():\n",
        "    reconstructed_class_examples, _, _, _ = AE_test(class_examples)\n",
        "    display_class_examples(class_examples)\n",
        "    display_class_examples(reconstructed_class_examples)\n",
        "\n",
        "    if counter % EPOCHS_BETWEEN_VAL == 0:\n",
        "      ############################\n",
        "      # Validate\n",
        "      ############################\n",
        "      val_losses = AE_epoch(testloader, AE_test, AE_test_optimizer,\n",
        "                            reconstruction_loss, train=False)\n",
        "\n",
        "      val_AE_test_losses.append(val_losses[0])\n",
        "      val_test_reconstruction_losses.append(val_losses[1])\n",
        "\n",
        "      ############################\n",
        "      # Display\n",
        "      ############################\n",
        "      print('Validation loss:{:.3f}, reconstruction:{:.3f}'.format(val_AE_test_losses[-1],\n",
        "                                                        val_test_reconstruction_losses[-1]))\n",
        "      reconstructed_val_class_examples, _, _, _ = AE_test(val_class_examples)\n",
        "      display_class_examples(val_class_examples)\n",
        "      display_class_examples(reconstructed_val_class_examples)\n",
        "\n",
        "  AE_test.train()\n",
        "  counter += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxXNU-pkITyZ"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "  i=0\n",
        "  for images, classes in testloader:\n",
        "    images = Variable(images).to(DEVICE)\n",
        "\n",
        "    reconstruction_test, encoded, _, _ = AE_test(images)\n",
        "    save_images(reconstruction_test, \"./reconstructed_test\", i)\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lq-YXvrMIdUq"
      },
      "outputs": [],
      "source": [
        "! python -m pytorch_fid ./original ./reconstructed_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ih3oH9Ljkpu0"
      },
      "source": [
        "#test cDCGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkroOSS0YTEt"
      },
      "outputs": [],
      "source": [
        "# Create the Discriminator\n",
        "D_test = Discriminator(10).to(DEVICE)\n",
        "print(D_test)\n",
        "D_test_total_params = sum(p.numel() for p in D_test.parameters() if p.requires_grad)\n",
        "print(f\"Total Trainable Parameters: {D_test_total_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVC6UpM-YZw0"
      },
      "outputs": [],
      "source": [
        "# Create the Generator\n",
        "G_test = Generator(10).to(DEVICE)\n",
        "print(G_test)\n",
        "G_test_total_params = sum(p.numel() for p in G_test.parameters() if p.requires_grad)\n",
        "print(f\"Total Trainable Parameters: {G_test_total_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiBI9XFPgKdq"
      },
      "outputs": [],
      "source": [
        "# custom weights initialization\n",
        "def weights_init(net):\n",
        "    classname = net.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(net.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(net.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(net.bias.data, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUGu769mgKdr"
      },
      "outputs": [],
      "source": [
        "# randomly initialize all weights to mean=0, stdev=0.2.\n",
        "D_test.apply(weights_init)\n",
        "G_test.apply(weights_init)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwqV4PE-Y0W9"
      },
      "outputs": [],
      "source": [
        "# Adam optimizer for generator\n",
        "optimizerG_test = torch.optim.Adam(G_test.parameters(), lr=Adam_lr, betas=(Adam_beta1, 0.999))\n",
        "# Adam optimizer for discriminator\n",
        "optimizerD_test = torch.optim.Adam(D_test.parameters(), lr=Adam_lr, betas=(Adam_beta1, 0.999))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsAS5bznZQBw"
      },
      "outputs": [],
      "source": [
        "# labels for training images x for Discriminator training\n",
        "labels_real = torch.ones((BATCH_SIZE, 1)).to(DEVICE)\n",
        "# labels for generated images G(z) for Discriminator training\n",
        "labels_fake = torch.zeros((BATCH_SIZE, 1)).to(DEVICE)\n",
        "# Fix noise for testing generator and visualization\n",
        "z_test = torch.randn(100, size_z).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbMH3KTRZS2c"
      },
      "outputs": [],
      "source": [
        "# convert labels to onehot encoding\n",
        "onehot = torch.zeros(10, 10).scatter_(1, torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]).view(10,1), 1)\n",
        "# reshape labels to image size, with number of labels as channel\n",
        "fill = torch.zeros([10, 10, img_size, img_size])\n",
        "#channel corresponding to label will be set one and all other zeros\n",
        "for i in range(10):\n",
        "    fill[i, i, :, :] = 1\n",
        "# create labels for testing generator\n",
        "test_y = torch.tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]*10).type(torch.LongTensor)\n",
        "# convert to one hot encoding\n",
        "test_Gy = onehot[test_y].to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xh7tOScWipfg"
      },
      "outputs": [],
      "source": [
        "class_examples_z = torch.randn(10, size_z).to(DEVICE)\n",
        "val_class_examples_z = torch.randn(10, size_z).to(DEVICE)\n",
        "print(class_examples_z.shape)\n",
        "\n",
        "# create a tensor of shape [10,10] with zeros everywhere\n",
        "label_examples = torch.zeros(10, 10).to(DEVICE)\n",
        "\n",
        "# set the diagonal elements to ones\n",
        "label_examples.diagonal().fill_(1)\n",
        "\n",
        "print(label_examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dygg3-_evECs"
      },
      "outputs": [],
      "source": [
        "def GAN_test_epoch(dataloader, D, D_optimizer, G, G_optimizer,\n",
        "              discrimination_loss, train=True):\n",
        "  torch.autograd.set_detect_anomaly(True)\n",
        "  epoch_D_losses = []\n",
        "  epoch_D_x_losses = []\n",
        "  epoch_D_z_losses = []\n",
        "  epoch_Dx = []\n",
        "\n",
        "  epoch_G_losses = []\n",
        "  epoch_G_z_losses = []\n",
        "  epoch_DGz = []\n",
        "\n",
        "  step = 0\n",
        "  # iterate through data loader generator object\n",
        "  for images, classes in dataloader:\n",
        "    images = Variable(images).to(DEVICE)\n",
        "\n",
        "    ############################\n",
        "    # Forward Pass Through Generator\n",
        "    ############################\n",
        "    # create latent vector z from normal distribution\n",
        "    z = torch.randn(BATCH_SIZE, size_z).to(DEVICE)\n",
        "    # create random y labels for generator\n",
        "    y_gen = (torch.rand(BATCH_SIZE, 1)*10).type(torch.LongTensor).squeeze()\n",
        "    # convert genarator labels to onehot\n",
        "    G_y = onehot[y_gen].to(DEVICE)\n",
        "    # preprocess labels for feeding as y input in D\n",
        "    # DG_y shape will be (batch_size, 10, 28, 28)\n",
        "    DG_y = fill[y_gen].to(DEVICE)\n",
        "\n",
        "    # generate image\n",
        "    fake_images = G(z, G_y)\n",
        "\n",
        "    ############################\n",
        "    # Calculate Discriminator loss on real images\n",
        "    ############################\n",
        "    # D_x shape will be (batch_size, ENCODED_DIM, 28, 28)\n",
        "    D_x = fill[classes].to(DEVICE)\n",
        "    # forward pass D(x)\n",
        "    x_preds = D(images, D_x)\n",
        "    # calculate loss log(D(x))\n",
        "    D_x_loss = discrimination_loss(x_preds, labels_real)\n",
        "\n",
        "    ############################\n",
        "    # Calculate Discriminator loss on fake images\n",
        "    ############################\n",
        "    # forward pass D(G(z))\n",
        "    z_preds = D(fake_images.detach(), DG_y)\n",
        "    # calculate loss log(1 - D(G(z)))\n",
        "    D_z_loss = discrimination_loss(z_preds, labels_fake)\n",
        "    ############################\n",
        "    # Update D network\n",
        "    ############################\n",
        "    D_loss = D_x_loss + D_z_loss\n",
        "\n",
        "    # save values for plots\n",
        "    epoch_D_losses.append(D_loss.item())\n",
        "    epoch_D_x_losses.append(D_x_loss.item())\n",
        "    epoch_D_z_losses.append(D_z_loss.item())\n",
        "    epoch_Dx.append(x_preds.mean().item())\n",
        "\n",
        "    if train:\n",
        "      # zero accumalted grads\n",
        "      D.zero_grad()\n",
        "      # do backward pass\n",
        "      D_loss.backward()\n",
        "      # update discriminator model\n",
        "      D_optimizer.step()\n",
        "\n",
        "    ############################\n",
        "    # Update G network\n",
        "    ############################\n",
        "    # if Ksteps of Discriminator training are done, update generator\n",
        "    if step % Ksteps == 0:\n",
        "      # As we done one step of discriminator, again calculate D(G(z))\n",
        "      # forward pass D(G(z))\n",
        "      z_out = D(fake_images, DG_y)\n",
        "      # calculate loss log(D(G(z)))\n",
        "      G_z_loss = discrimination_loss(z_out, labels_real)\n",
        "      # Calculate the Mean Squared Error loss between the original and generated image\n",
        "      G_z_reconstruction_loss = reconstruction_loss(fake_images, images)\n",
        "\n",
        "      G_loss = G_z_loss\n",
        "\n",
        "      # save values for plots\n",
        "      epoch_G_losses.append(G_loss.item())\n",
        "      epoch_G_z_losses.append(G_z_loss.item())\n",
        "      epoch_DGz.append(z_out.mean().item())\n",
        "\n",
        "      if train:\n",
        "        # zero accumalted grads\n",
        "        G.zero_grad()\n",
        "        # do backward pass\n",
        "        G_loss.backward()\n",
        "        # update generator model\n",
        "        G_optimizer.step()\n",
        "    step += 1\n",
        "\n",
        "  ############################\n",
        "  # Log\n",
        "  ############################\n",
        "  epoch_D_loss = sum(epoch_D_losses)/ len(epoch_D_losses)\n",
        "  epoch_D_x_loss = sum(epoch_D_x_losses)/ len(epoch_D_x_losses)\n",
        "  epoch_D_z_loss = sum(epoch_D_z_losses)/ len(epoch_D_z_losses)\n",
        "  epoch_Dx = sum(epoch_Dx)/ len(epoch_Dx)\n",
        "\n",
        "  epoch_G_loss = sum(epoch_G_losses)/ len(epoch_G_losses)\n",
        "  epoch_G_z_loss = sum(epoch_G_z_losses)/ len(epoch_G_z_losses)\n",
        "  epoch_DGz = sum(epoch_DGz)/ len(epoch_DGz)\n",
        "\n",
        "  return epoch_D_loss, epoch_D_x_loss, epoch_D_z_loss, epoch_Dx, \\\n",
        "         epoch_G_loss, epoch_G_z_loss, \\\n",
        "         epoch_DGz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoQDbj00whtT"
      },
      "outputs": [],
      "source": [
        "if PRE_TRAIN or ARCHITECHTURE == 'cDCGAN' or ARCHITECHTURE == 'VAE and cDCGAN':\n",
        "  # List of values, which will be used for plotting purpose\n",
        "  D_losses = []\n",
        "  D_x_losses = []\n",
        "  D_z_losses = []\n",
        "  Dx_values = []\n",
        "\n",
        "  G_losses = []\n",
        "  G_z_losses = []\n",
        "  DGz_values = []\n",
        "\n",
        "  val_D_losses = []\n",
        "  val_D_x_losses = []\n",
        "  val_D_z_losses = []\n",
        "  val_Dx_values = []\n",
        "\n",
        "  val_G_losses = []\n",
        "  val_G_z_losses = []\n",
        "  val_DGz_values = []\n",
        "\n",
        "  counter = 0\n",
        "  for epoch in range(NUM_EPOCHS + NUM_PRETRAIN_EPOCHS):\n",
        "    ############################\n",
        "    # Train\n",
        "    ############################\n",
        "    losses = GAN_test_epoch(dataloader, D_test, optimizerD_test, G_test, optimizerG_test,\n",
        "                                  discrimination_loss,\n",
        "                                  train=True)\n",
        "\n",
        "    D_losses.append(losses[0])\n",
        "    D_x_losses.append(losses[1])\n",
        "    D_z_losses.append(losses[2])\n",
        "    Dx_values.append(losses[3])\n",
        "\n",
        "    G_losses.append(losses[4])\n",
        "    G_z_losses.append(losses[5])\n",
        "    DGz_values.append(losses[6])\n",
        "\n",
        "    ############################\n",
        "    # Display\n",
        "    ############################\n",
        "    G.eval()\n",
        "    D.eval()\n",
        "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS + NUM_PRETRAIN_EPOCHS} Discriminator Loss {D_losses[-1]:.3f} \"\n",
        "        + f\"Generator Loss {G_losses[-1]:.3f} \"\n",
        "        + f\"D(x) {Dx_values[-1]:.3f} D(G(z)) {DGz_values[-1]:.3f}\")\n",
        "    with torch.no_grad():\n",
        "      fake_class_examples = G_test(class_examples_z, label_examples)\n",
        "      display_class_examples(fake_class_examples)\n",
        "\n",
        "      if counter % EPOCHS_BETWEEN_VAL == 0:\n",
        "        ############################\n",
        "        # Validate\n",
        "        ############################\n",
        "        val_losses = GAN_test_epoch(testloader, D_test, optimizerD_test, G_test, optimizerG_test,\n",
        "                                                  discrimination_loss,\n",
        "                                                  train=False)\n",
        "\n",
        "        val_D_losses.append(losses[0])\n",
        "        val_D_x_losses.append(losses[1])\n",
        "        val_D_z_losses.append(losses[2])\n",
        "        val_Dx_values.append(losses[3])\n",
        "\n",
        "        val_G_losses.append(losses[4])\n",
        "        val_G_z_losses.append(losses[5])\n",
        "        val_DGz_values.append(losses[6])\n",
        "\n",
        "        ############################\n",
        "        # Display\n",
        "        ############################\n",
        "        print(f\"Validation Discriminator Loss {val_D_losses[-1]:.3f} \"\n",
        "            + f\"Generator Loss {val_G_losses[-1]:.3f} \"\n",
        "            + f\"D(x) {val_Dx_values[-1]:.3f} D(G(x)) {val_DGz_values[-1]:.3f}\")\n",
        "        fake_val_class_examples = G_test(val_class_examples_z, label_examples)\n",
        "        display_class_examples(fake_val_class_examples)\n",
        "\n",
        "    D.train()\n",
        "    G.train()\n",
        "    counter += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_uB9l3YyswC"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "  i=0\n",
        "  for images, classes in testloader:\n",
        "    images = Variable(images).to(DEVICE)\n",
        "\n",
        "\n",
        "    # create latent vector z from normal distribution\n",
        "    z_test = torch.randn(BATCH_SIZE, size_z).to(DEVICE)\n",
        "    # create random y labels for generator\n",
        "    y_gen = (torch.rand(BATCH_SIZE, 1)*10).type(torch.LongTensor).squeeze()\n",
        "    # convert genarator labels to onehot\n",
        "    G_y = onehot[y_gen].to(DEVICE)\n",
        "    fake_images_test = G_test(z_test, G_y)\n",
        "    save_images(fake_images_test, \"./generated_test\", i)\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWc1oF_zzmJW"
      },
      "outputs": [],
      "source": [
        "! python -m pytorch_fid ./original ./generated_test"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "75T1NtI4o6ha"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}